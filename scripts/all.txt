The following contains a tree of the directory and all the important files of the current version of my project: 

================================================================================

FILE NAME: scripts/tree_output.txt

.
├── README.md
├── __pycache__
│   ├── config.cpython-312.pyc
│   └── run.cpython-312.pyc
├── app
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── __init__.cpython-312.pyc
│   │   ├── models.cpython-312.pyc
│   │   ├── routes.cpython-312.pyc
│   │   └── utils.cpython-312.pyc
│   ├── models.py
│   ├── routes.py
│   ├── static
│   ├── templates
│   └── utils.py
├── config.py
├── dev_database.db
├── join_files.py
├── migrations
│   ├── README
│   ├── __pycache__
│   │   └── env.cpython-312.pyc
│   ├── alembic.ini
│   ├── env.py
│   ├── script.py.mako
│   └── versions
│       ├── 248ae2c313bd_initial_migration_for_development_sqlite.py
│       ├── 370e414ac95f_initial_migration_for_development_sqlite.py
│       └── __pycache__
│           ├── 248ae2c313bd_initial_migration_for_development_sqlite.cpython-312.pyc
│           └── 370e414ac95f_initial_migration_for_development_sqlite.cpython-312.pyc
├── requirements.txt
├── run.py
├── scripts
├── tests
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── __init__.cpython-312.pyc
│   │   ├── conftest.cpython-312-pytest-8.3.4.pyc
│   │   ├── fixtures.cpython-312.pyc
│   │   ├── test_initial_database_population.cpython-312-pytest-8.3.4.pyc
│   │   ├── test_keywords.cpython-312-pytest-8.3.4.pyc
│   │   ├── test_relationships.cpython-312-pytest-8.3.4.pyc
│   │   ├── test_save_models.cpython-312-pytest-8.3.4.pyc
│   │   ├── test_save_modules.cpython-312-pytest-8.3.4.pyc
│   │   └── test_timeseries.cpython-312-pytest-8.3.4.pyc
│   ├── conftest.py
│   ├── fixtures.py
│   ├── test_initial_database_population.py
│   ├── test_keywords.py
│   ├── test_relationships.py
│   ├── test_save_models.py
│   └── test_timeseries.py
└── tutorial
    └── save_time_series.ipynb

14 directories, 43 files


================================================================================

FILE NAME: scripts/migrations.txt

Migrations files:

================================================================================

FILE NAME: migrations/env.py

import logging
from logging.config import fileConfig

from flask import current_app

from alembic import context

# this is the Alembic Config object, which provides
# access to the values within the .ini file in use.
config = context.config

# Interpret the config file for Python logging.
# This line sets up loggers basically.
fileConfig(config.config_file_name)
logger = logging.getLogger('alembic.env')


def get_engine():
    try:
        # this works with Flask-SQLAlchemy<3 and Alchemical
        return current_app.extensions['migrate'].db.get_engine()
    except (TypeError, AttributeError):
        # this works with Flask-SQLAlchemy>=3
        return current_app.extensions['migrate'].db.engine


def get_engine_url():
    try:
        return get_engine().url.render_as_string(hide_password=False).replace(
            '%', '%%')
    except AttributeError:
        return str(get_engine().url).replace('%', '%%')


# add your model's MetaData object here
# for 'autogenerate' support
# from myapp import mymodel
# target_metadata = mymodel.Base.metadata
config.set_main_option('sqlalchemy.url', get_engine_url())
target_db = current_app.extensions['migrate'].db

# other values from the config, defined by the needs of env.py,
# can be acquired:
# my_important_option = config.get_main_option("my_important_option")
# ... etc.


def get_metadata():
    if hasattr(target_db, 'metadatas'):
        return target_db.metadatas[None]
    return target_db.metadata


def run_migrations_offline():
    """Run migrations in 'offline' mode.

    This configures the context with just a URL
    and not an Engine, though an Engine is acceptable
    here as well.  By skipping the Engine creation
    we don't even need a DBAPI to be available.

    Calls to context.execute() here emit the given string to the
    script output.

    """
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url, target_metadata=get_metadata(), literal_binds=True
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online():
    """Run migrations in 'online' mode.

    In this scenario we need to create an Engine
    and associate a connection with the context.

    """

    # this callback is used to prevent an auto-migration from being generated
    # when there are no changes to the schema
    # reference: http://alembic.zzzcomputing.com/en/latest/cookbook.html
    def process_revision_directives(context, revision, directives):
        if getattr(config.cmd_opts, 'autogenerate', False):
            script = directives[0]
            if script.upgrade_ops.is_empty():
                directives[:] = []
                logger.info('No changes in schema detected.')

    conf_args = current_app.extensions['migrate'].configure_args
    if conf_args.get("process_revision_directives") is None:
        conf_args["process_revision_directives"] = process_revision_directives

    connectable = get_engine()

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=get_metadata(),
            **conf_args
        )

        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()


================================================================================

FILE NAME: migrations/alembic.ini

# A generic, single database configuration.

[alembic]
# template used to generate migration files
# file_template = %%(rev)s_%%(slug)s

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false


# Logging configuration
[loggers]
keys = root,sqlalchemy,alembic,flask_migrate

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARN
handlers = console
qualname =

[logger_sqlalchemy]
level = WARN
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[logger_flask_migrate]
level = INFO
handlers =
qualname = flask_migrate

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S


================================================================================



================================================================================

FILE NAME: scripts/tests.txt

Test files:

================================================================================

FILE NAME: tests/conftest.py

# tests/conftest.py

import pytest
import sys
import pandas as pd
import os

# Add the project root to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app import create_app, db


@pytest.fixture(scope='function')
def app():
    """
    Create and configure a new app instance for each test.
    """
    # Set the environment variable to 'testing'
    os.environ['FLASK_ENV'] = 'testing'
    
    # Create the Flask app with testing configuration
    app = create_app('testing')
    
    # Establish an application context
    with app.app_context():
        # Create the database and the database tables
        db.create_all()
        yield app
        # Drop the database tables after the test
        db.session.remove()
        db.drop_all()


@pytest.fixture(scope='function')
def client(app):
    """
    Create a test client for the app.
    """
    return app.test_client()


@pytest.fixture(scope='function')
def runner(app):
    """
    Create a CLI runner for the app.
    """
    return app.test_cli_runner()


# Import fixtures from fixtures.py
from .fixtures import (
    populate_test_db,
    sample_df_single_column,    
    sample_df_multiple_columns,
    create_seriesgroup_and_type  # Updated fixture name
)

================================================================================

FILE NAME: tests/test_keywords.py

# tests/test_keywords.py

import pytest
from sqlalchemy.exc import IntegrityError, InvalidRequestError
from sqlalchemy.orm import joinedload
from sqlalchemy import select
from app.models import SeriesGroup, Keyword, SeriesBase
from app import db

@pytest.fixture
def seriesgroup(app):
    """
    Fixture to create a SeriesGroup without keywords.
    """
    sg = SeriesGroup(
        name="SG_Test",
        description="Test SeriesGroup",
        series_group_code="SG999"
    )
    sg.save()
    return sg

def test_add_keywords_during_creation(app):
    """
    Test creating a SeriesGroup and adding keywords after saving.
    Verify that keywords are saved and associated correctly.
    """
    keywords = ["Finance", "Investment", "Portfolio"]
    sg = SeriesGroup(
        name="SG_WithKeywords",
        description="SeriesGroup with initial keywords",
        series_group_code="SG100"
        # Do not pass keywords here to avoid DetachedInstanceError
    )
    sg.save()

    # Now, add keywords after the object is bound to the session
    for kw in keywords:
        sg.add_keyword(kw)
    sg.save()

    # Retrieve the SeriesGroup from the database with keywords eagerly loaded
    stmt = select(SeriesGroup).options(joinedload(SeriesGroup.keywords)).where(SeriesGroup.id == sg.id)
    retrieved_sg = db.session.execute(stmt).scalars().unique().one()

    assert retrieved_sg is not None, "SeriesGroup should be saved and retrievable."

    # Verify keywords are associated
    associated_keywords = [kw.word for kw in retrieved_sg.keywords]
    assert set(associated_keywords) == set(keywords), "Keywords should match the ones provided during creation."

def test_add_keywords_later(app, seriesgroup):
    """
    Test adding keywords to a SeriesGroup after it has been created.
    Verify that keywords are saved and associated correctly.
    """
    # Initially, the SeriesGroup has no keywords
    stmt = select(SeriesGroup).options(joinedload(SeriesGroup.keywords)).where(SeriesGroup.id == seriesgroup.id)
    retrieved_sg = db.session.execute(stmt).scalars().unique().one()
    assert len(retrieved_sg.keywords) == 0, "SeriesGroup should initially have no keywords."

    # Add keywords
    new_keywords = ["Stocks", "Bonds"]
    for kw in new_keywords:
        retrieved_sg.add_keyword(kw)

    # Save the changes
    retrieved_sg.save()

    # Retrieve again with eager loading
    retrieved_sg = db.session.execute(stmt).scalars().unique().one()

    # Verify keywords are associated
    associated_keywords = [kw.word for kw in retrieved_sg.keywords]
    assert set(associated_keywords) == set(new_keywords), "Keywords should match the ones added later."

def test_add_duplicate_keywords(app, seriesgroup):
    """
    Test that adding duplicate keywords does not create duplicate entries in the database.
    """
    keyword = "Equity"
    # Add the keyword for the first time
    seriesgroup.add_keyword(keyword)
    seriesgroup.save()

    # Attempt to add the same keyword again
    seriesgroup.add_keyword(keyword)
    seriesgroup.save()

    # Retrieve with eager loading
    stmt = select(SeriesGroup).options(joinedload(SeriesGroup.keywords)).where(SeriesGroup.id == seriesgroup.id)
    retrieved_sg = db.session.execute(stmt).scalars().unique().one()

    # Verify only one instance of the keyword exists
    assert len(retrieved_sg.keywords) == 1, "Duplicate keywords should not be added."
    assert retrieved_sg.keywords[0].word == keyword, "Keyword should match the one added."

def test_add_invalid_keyword_length(app, seriesgroup):
    """
    Test that adding a keyword exceeding the maximum length raises a ValueError.
    """
    invalid_keyword = "K" * 51  # Exceeds the 50 character limit

    with pytest.raises(ValueError) as exc_info:
        # Attempt to add an overly long keyword
        seriesgroup.add_keyword(invalid_keyword)
        seriesgroup.save()
    # No need to rollback since the exception is handled

    assert "Keyword must be 50 characters or less." in str(exc_info.value), \
        "Adding a keyword with invalid length should raise a ValueError."

def test_remove_keyword(app, seriesgroup):
    """
    Test removing a keyword from a SeriesGroup.
    """
    keywords = ["Growth", "Value", "Income"]
    for kw in keywords:
        seriesgroup.add_keyword(kw)
    seriesgroup.save()

    # Retrieve with eager loading
    stmt = select(SeriesGroup).options(joinedload(SeriesGroup.keywords)).where(SeriesGroup.id == seriesgroup.id)
    retrieved_sg = db.session.execute(stmt).scalars().unique().one()

    assert len(retrieved_sg.keywords) == 3, "SeriesGroup should have three keywords initially."

    # Remove one keyword
    retrieved_sg.remove_keyword("Value")
    retrieved_sg.save()

    # Retrieve again with eager loading
    retrieved_sg = db.session.execute(stmt).scalars().unique().one()

    # Verify the keyword is removed
    associated_keywords = [kw.word for kw in retrieved_sg.keywords]
    assert "Value" not in associated_keywords, "Keyword 'Value' should be removed."
    assert len(retrieved_sg.keywords) == 2, "SeriesGroup should have two keywords after removal."

def test_add_keyword_with_invalid_type(app, seriesgroup):
    """
    Test that adding a keyword with an invalid type (non-string) raises a TypeError.
    """
    invalid_keyword = 12345  # Non-string keyword

    with pytest.raises(TypeError) as exc_info:
        # Attempt to add a non-string keyword
        seriesgroup.add_keyword(invalid_keyword)
        seriesgroup.save()
    # No need to rollback since the exception is handled

    assert "Keyword must be a string." in str(exc_info.value), \
        "Adding a keyword with invalid type should raise a TypeError."

def test_save_and_retrieve_keywords(app):
    """
    Test saving a SeriesGroup with keywords and retrieving it to verify persistence.
    """
    keywords = ["Alpha", "Beta", "Gamma"]
    sg = SeriesGroup(
        name="SG_Persistence",
        description="SeriesGroup for persistence test",
        series_group_code="SG200"
        # Do not pass keywords here to avoid DetachedInstanceError
    )
    sg.save()

    # Now, add keywords after the object is bound to the session
    for kw in keywords:
        sg.add_keyword(kw)
    sg.save()

    # Retrieve the SeriesGroup from the database with keywords eagerly loaded
    stmt = select(SeriesGroup).options(joinedload(SeriesGroup.keywords)).where(SeriesGroup.id == sg.id)
    retrieved_sg = db.session.execute(stmt).scalars().unique().one()

    assert retrieved_sg is not None, "SeriesGroup should be retrievable after saving."

    # Verify keywords are persisted
    associated_keywords = [kw.word for kw in retrieved_sg.keywords]
    assert set(associated_keywords) == set(keywords), "Persisted keywords should match the ones saved."

def test_save_keywords_added_later(app, seriesgroup):
    """
    Test saving keywords added after the initial creation and verifying persistence.
    """
    # Add keywords after creation
    keywords = ["Delta", "Epsilon"]
    for kw in keywords:
        seriesgroup.add_keyword(kw)
    seriesgroup.save()

    # Retrieve the SeriesGroup with keywords eagerly loaded
    stmt = select(SeriesGroup).options(joinedload(SeriesGroup.keywords)).where(SeriesGroup.id == seriesgroup.id)
    retrieved_sg = db.session.execute(stmt).scalars().unique().one()

    assert retrieved_sg is not None, "SeriesGroup should be retrievable after adding keywords."

    # Verify keywords are persisted
    associated_keywords = [kw.word for kw in retrieved_sg.keywords]
    assert set(associated_keywords) == set(keywords), "Persisted keywords should match the ones added later."

================================================================================

FILE NAME: tests/test_timeseries.py

# tests/test_timeseries.py

import random
import string
import pytest
import datetime
import pandas as pd
from app.models import TimeSeries, DataPoint, SeriesGroup, TimeSeriesType
from app import db  # Import db for database operations


def test_from_dataframe_single_column(
    app,
    sample_df_single_column,
    create_seriesgroup_and_type  # Updated fixture name
):
    """
    Test that from_dataframe handles a single-column DataFrame and returns one TimeSeries.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    with app.app_context():
        ts_obj = TimeSeries.from_dataframe(
            df=sample_df_single_column,
            code='AAPL',  # Add a code for the TimeSeries
            series_groups=[seriesgroup],  # Pass as a list of objects
            time_series_type=tstype  # Assign the object directly
        )

        # Expect a single TimeSeries (not a list)
        assert isinstance(ts_obj, TimeSeries), "Should return a single TimeSeries object."
        assert ts_obj.name == "price", "TimeSeries name should match the DataFrame column name."
        assert len(ts_obj.data_points) == 252, "TimeSeries should have 252 DataPoints."

        # Check the data points
        for i, dp in enumerate(ts_obj.data_points):
            expected_date = sample_df_single_column.index[i].date()
            expected_value = sample_df_single_column["price"].iloc[i]
            assert dp.date.date() == expected_date, f"Expected date {expected_date}, got {dp.date}"
            assert dp.value == expected_value, f"Expected value {expected_value}, got {dp.value}"


def test_from_dataframe_single_column_with_seriesgroup_and_tstype(
    app,
    sample_df_single_column,
    create_seriesgroup_and_type  # Updated fixture name
):
    """
    Test that from_dataframe handles a single-column DataFrame and returns one TimeSeries.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    with app.app_context():
        ts_obj = TimeSeries.from_dataframe(
            df=sample_df_single_column,
            code='AAPL',
            series_groups=[seriesgroup],  # Pass as a list of objects
            time_series_type=tstype  # Assign the object directly
        )

        # Expect a single TimeSeries (not a list)
        assert isinstance(ts_obj, TimeSeries), "Should return a single TimeSeries object."
        assert ts_obj.name == "price", "TimeSeries name should match the DataFrame column name."
        assert len(ts_obj.data_points) == 252, "TimeSeries should have 252 DataPoints."

        # Check the data points
        for i, dp in enumerate(ts_obj.data_points):
            expected_date = sample_df_single_column.index[i].date()
            expected_value = sample_df_single_column["price"].iloc[i]
            assert dp.date.date() == expected_date, f"Expected date {expected_date}, got {dp.date}"
            assert dp.value == expected_value, f"Expected value {expected_value}, got {dp.value}"


def test_from_dataframe_single_column_with_date_as_column(
    app,
    sample_df_single_column,
    create_seriesgroup_and_type  # Updated fixture name
):
    """
    Test that from_dataframe handles a single-column DataFrame and returns one TimeSeries with a specified date column.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    sample_df = sample_df_single_column.copy()
    sample_df["Datetime"] = sample_df.index
    sample_df = sample_df.reset_index(drop=True)

    with app.app_context():
        ts_obj = TimeSeries.from_dataframe(
            df=sample_df,
            series_groups=[seriesgroup],  # Pass as a list of objects
            time_series_type=tstype,  # Assign the object directly
            code='AAPL',
            date_column="Datetime"
        )

        # Expect a single TimeSeries (not a list)
        assert isinstance(ts_obj, TimeSeries), "Should return a single TimeSeries object."
        assert ts_obj.name == "price", "TimeSeries name should match the DataFrame column name."
        assert len(ts_obj.data_points) == 252, "TimeSeries should have 252 DataPoints."

        # Check the data points
        for i, dp in enumerate(ts_obj.data_points):
            expected_date = sample_df["Datetime"][i].date()
            expected_value = sample_df["price"].iloc[i]
            assert dp.date.date() == expected_date, f"Expected date {expected_date}, got {dp.date}"
            assert dp.value == expected_value, f"Expected value {expected_value}, got {dp.value}"


def test_from_dataframe_multi_column(
    app,
    sample_df_multiple_columns,
    create_seriesgroup_and_type  # Updated fixture name
):
    """
    Test that from_dataframe handles a multi-column DataFrame and returns multiple TimeSeries.
    """
    seriesgroup, tstype = create_seriesgroup_and_type
    num_columns = len(sample_df_multiple_columns.columns)

    with app.app_context():
        # Since there are multiple columns, provide a list of SeriesGroup objects
        # For simplicity, using the same SeriesGroup for all columns
        series_groups = [seriesgroup for _ in range(num_columns)]
        codes = [''.join(random.choices(string.ascii_uppercase, k=3)) for _ in range(num_columns)]

        ts_objs = TimeSeries.from_dataframe(
            df=sample_df_multiple_columns,
            series_groups=series_groups,  # Pass as a list of objects
            time_series_type=tstype,  # Assign the object directly
            code=codes  # Add a code for all TimeSeries
        )

        # Expect a list of TimeSeries objects
        assert isinstance(ts_objs, list), "Should return a list of TimeSeries objects."
        assert len(ts_objs) == num_columns, "Should return one TimeSeries per column."
        assert all(isinstance(ts, TimeSeries) for ts in ts_objs), "All elements should be TimeSeries objects."
        assert ts_objs[0].name == "price", "First TimeSeries name should match the first DataFrame column name."


def test_to_dataframe_multi_column(
    app,
    sample_df_multiple_columns,
    create_seriesgroup_and_type  # Updated fixture name
):
    """
    Test that from_dataframe handles a multi-column DataFrame and the resulting TimeSeries can be converted back to DataFrame.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    with app.app_context():
        # Provide a list of SeriesGroup objects matching the number of columns
        num_columns = len(sample_df_multiple_columns.columns)
        series_groups = [seriesgroup for _ in range(num_columns)]
        codes = [''.join(random.choices(string.ascii_uppercase, k=3)) for _ in range(num_columns)]

        ts_objs = TimeSeries.from_dataframe(
            df=sample_df_multiple_columns,
            series_groups=series_groups,  # Pass as a list of objects
            time_series_type=tstype,  # Assign the object directly
            code=codes  # Add a code for all TimeSeries
        )
        for ts_obj in ts_objs:
            ts_dataframe = ts_obj.to_dataframe()
            assert isinstance(ts_dataframe, pd.DataFrame), "Should return a DataFrame."
            assert len(ts_dataframe.index) == len(sample_df_multiple_columns.index), "DataFrame index length should match input length."


def test_to_dataframe_single_column(
    app,
    sample_df_single_column,
    create_seriesgroup_and_type  # Updated fixture name
):
    """
    Test that from_dataframe handles a single-column DataFrame and returns one TimeSeries, which can be converted back to DataFrame.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    with app.app_context():
        ts_obj = TimeSeries.from_dataframe(
            df=sample_df_single_column,
            series_groups=[seriesgroup],  # Pass as a list of objects
            time_series_type=tstype,  # Assign the object directly
            code='AAPL'
        )

        ts_dataframe = ts_obj.to_dataframe()
        assert isinstance(ts_dataframe, pd.DataFrame), "Should return a DataFrame."
        assert len(ts_dataframe.index) == len(sample_df_single_column.index), "DataFrame index length should match input length."
        assert list(ts_dataframe.columns) == ["price"], "DataFrame columns should match the input DataFrame."
        for i in range(5):
            assert ts_dataframe.iloc[i, 0] == sample_df_single_column.iloc[i, 0], "DataFrame values should match the input DataFrame."

================================================================================

FILE NAME: tests/test_initial_database_population.py

# tests/test_initial_database_population.py

import pytest
from app.models import SeriesGroup, TimeSeriesType, TimeSeries, DataPoint
import datetime

def test_initial_population(populate_test_db):
    """
    Test the initial population of the test database by verifying the created entries.
    """
    # Populate the database with 3 SeriesGroups and 5 data points per time series
    populate_test_db(num_seriesgroups=3, num_data_points=5, start_date="2024-01-01")

    # Verify the number of entries created
    assert SeriesGroup.query.count() == 3, "There should be exactly 3 SeriesGroups."
    assert TimeSeriesType.query.count() == 1, "There should be exactly one TimeSeriesType."
    assert TimeSeries.query.count() == 3, "There should be exactly three TimeSeries."
    assert DataPoint.query.count() == 15, "Each TimeSeries should have 5 DataPoints."

    # Retrieve and check the SeriesGroups
    for sg in SeriesGroup.query.all():
        assert len(sg.name) == 3, f"SeriesGroup name '{sg.name}' should have exactly three characters."
        assert sg.description.endswith("Description"), f"SeriesGroup description mismatch for '{sg.name}'."
        assert sg.series_group_code is not None and len(sg.series_group_code) == 5, (
            f"SeriesGroup '{sg.name}' should have a valid series_group_code of length 5."
        )

    # Retrieve and check the TimeSeriesType
    time_series_type = TimeSeriesType.query.first()
    assert time_series_type.name == 'Price', "TimeSeriesType name should be 'Price'."
    assert time_series_type.description == 'Price Time Series', "TimeSeriesType description mismatch."

    # Retrieve and check the TimeSeries and their DataPoints
    for time_series in TimeSeries.query.all():
        assert len(time_series.name) > 0, "TimeSeries name should not be empty."
        assert time_series.type_id == time_series_type.id, "TimeSeries type_id mismatch."

        # Ensure each TimeSeries is associated with at least one SeriesGroup
        associated_sg_ids = [sg.id for sg in time_series.series_groups]
        assert len(associated_sg_ids) >= 1, (
            f"TimeSeries '{time_series.name}' should be associated with at least one SeriesGroup."
        )

        data_points = time_series.data_points
        assert len(data_points) == 5, f"TimeSeries '{time_series.name}' should have exactly 5 DataPoints."

        # Verify the DataPoints
        start_date = datetime.date(2024, 1, 1)
        for i, dp in enumerate(data_points):
            expected_date = start_date + datetime.timedelta(days=i)
            assert dp.date == expected_date, (
                f"DataPoint date mismatch: expected {expected_date}, got {dp.date}."
            )
            assert isinstance(dp.value, float), "DataPoint 'value' should be a float."
            assert dp.time_series_id == time_series.id, (
                f"DataPoint time_series_id mismatch for DataPoint ID {dp.id}."
            )

            # Check `date_release` conditionally
            if dp.date_release:
                expected_release_date = dp.date + datetime.timedelta(days=1)
                assert dp.date_release == expected_release_date, (
                    f"DataPoint 'date_release' mismatch: expected {expected_release_date}, got {dp.date_release}."
                )

================================================================================

FILE NAME: tests/__init__.py



================================================================================

FILE NAME: tests/test_relationships.py

# tests/test_relationships.py

import pytest
import datetime
from app import db
from app.models import SeriesGroup, TimeSeries, SeriesBase, TimeSeriesType, Keyword, DataPoint

@pytest.fixture
def create_parent_child_seriesgroups(app):
    """
    Fixture to create a parent SeriesGroup with multiple child SeriesGroups.
    """
    parent = SeriesGroup(name="ParentSG", description="Parent SeriesGroup", series_group_code="PSG001")
    child1 = SeriesGroup(name="ChildSG1", description="First Child SeriesGroup", series_group_code="CSG001", parent=parent)
    child2 = SeriesGroup(name="ChildSG2", description="Second Child SeriesGroup", series_group_code="CSG002", parent=parent)
    db.session.add_all([parent, child1, child2])
    db.session.commit()
    return parent, child1, child2

def test_seriesgroup_parent_child_relationship(app, create_parent_child_seriesgroups):
    """
    Test the self-referential parent-child relationship in SeriesGroup.
    """
    parent, child1, child2 = create_parent_child_seriesgroups

    # Verify parent has two children
    assert len(parent.children.all()) == 2, "Parent SeriesGroup should have two children."
    assert child1.parent == parent, "Child1's parent should be ParentSG."
    assert child2.parent == parent, "Child2's parent should be ParentSG."

    # Verify children are correctly linked
    assert child1 in parent.children.all(), "Child1 should be in ParentSG's children."
    assert child2 in parent.children.all(), "Child2 should be in ParentSG's children."

def test_polymorphic_querying(app):
    """
    Test that querying SeriesBase returns instances of the correct subclasses.
    """
    tstype = TimeSeriesType(name="Polymorphic", description="Polymorphic TimeSeriesType")

    # Create a SeriesGroup
    sg = SeriesGroup(name="SG_Poly", description="Polymorphic SeriesGroup", series_group_code="SGP001")
    db.session.add(sg)

    # Create a TimeSeries
    ts = TimeSeries(name="TS_Poly", time_series_type=tstype, code="TSP001")
    db.session.add(ts)

    db.session.commit()

    # Query SeriesBase
    all_seriesbase = SeriesBase.query.all()
    assert len(all_seriesbase) == 2, "There should be three SeriesBase instances."
    
    # Optionally, verify the types of each instance
    assert any(isinstance(sb, SeriesGroup) for sb in all_seriesbase), "At least one SeriesGroup should be present."
    assert any(isinstance(sb, TimeSeries) for sb in all_seriesbase), "At least one TimeSeries should be present."

def test_seriesgroup_seriesbase_association(app, create_seriesgroup_and_type):
    """
    Test the many-to-many relationship between SeriesGroup and SeriesBase.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    # Create additional SeriesBase instances
    ts1 = TimeSeries(name="TS_Assoc1", time_series_type=tstype, code="TSA001")
    ts2 = TimeSeries(name="TS_Assoc2", time_series_type=tstype, code="TSA002")
    db.session.add_all([ts1, ts2])
    db.session.commit()

    # Associate SeriesBase with SeriesGroup
    seriesgroup.series.append(ts1)
    seriesgroup.series.append(ts2)
    db.session.commit()

    # Verify associations
    assert ts1 in seriesgroup.series.all(), "TS_Assoc1 should be associated with SeriesGroup."
    assert ts2 in seriesgroup.series.all(), "TS_Assoc2 should be associated with SeriesGroup."
    assert seriesgroup in ts1.series_groups.all(), "SeriesGroup should be associated with TS_Assoc1."
    assert seriesgroup in ts2.series_groups.all(), "SeriesGroup should be associated with TS_Assoc2."

def test_keyword_uniqueness(app, create_seriesgroup_and_type):
    """
    Test that adding duplicate keywords raises an IntegrityError.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    # Add a keyword
    keyword = Keyword(word="Finance")
    db.session.add(keyword)
    db.session.commit()

    # Attempt to add the same keyword again
    duplicate_keyword = Keyword(word="Finance")
    db.session.add(duplicate_keyword)

    with pytest.raises(Exception) as exc_info:
        db.session.commit()

    db.session.rollback()
    assert 'UNIQUE constraint failed' in str(exc_info.value), "Adding duplicate keyword should fail due to unique constraint."

def test_seriesgroup_unique_code(app, create_seriesgroup_and_type):
    """
    Test that SeriesGroup's series_group_code is unique.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    # Create a SeriesGroup with a unique code
    sg_unique = SeriesGroup(name="UniqueSG", description="Unique Code SeriesGroup", series_group_code="USG001")
    db.session.add(sg_unique)
    db.session.commit()

    # Attempt to create another SeriesGroup with the same code
    sg_duplicate = SeriesGroup(name="DuplicateSG", description="Duplicate Code SeriesGroup", series_group_code="USG001")
    db.session.add(sg_duplicate)

    with pytest.raises(Exception) as exc_info:
        db.session.commit()

    db.session.rollback()
    assert 'UNIQUE constraint failed' in str(exc_info.value), "Adding SeriesGroup with duplicate code should fail due to unique constraint."

def test_time_series_unique_code(app, create_seriesgroup_and_type):
    """
    Test that TimeSeries's time_series_code is unique.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    # Create a TimeSeries with a unique code
    ts_unique = TimeSeries(name="UniqueTS", time_series_type=tstype, code="UTS001")
    db.session.add(ts_unique)
    db.session.commit()

    # Attempt to create another TimeSeries with the same code
    ts_duplicate = TimeSeries(name="DuplicateTS", time_series_type=tstype, code="UTS001")
    db.session.add(ts_duplicate)

    with pytest.raises(Exception) as exc_info:
        db.session.commit()

    db.session.rollback()
    assert 'UNIQUE constraint failed' in str(exc_info.value), "Adding TimeSeries with duplicate code should fail due to unique constraint."

def test_seriesbase_deletion_cascade(app, create_seriesgroup_and_type):
    """
    Test that deleting a SeriesBase (TimeSeries) also deletes associated DataPoints if cascade is set.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    # Create a TimeSeries with DataPoints
    ts = TimeSeries(name="TS_DeleteCascade", time_series_type=tstype, code="TSC001")
    dp1 = DataPoint(date=datetime.date(2025, 1, 1), value=100.0, time_series=ts)
    dp2 = DataPoint(date=datetime.date(2025, 1, 2), value=110.0, time_series=ts)
    db.session.add_all([ts, dp1, dp2])
    db.session.commit()

    # Verify DataPoints exist
    assert DataPoint.query.filter_by(time_series_id=ts.id).count() == 2, "There should be two DataPoints associated with the TimeSeries."

    # Delete the TimeSeries
    db.session.delete(ts)
    db.session.commit()

    # Verify TimeSeries is deleted
    assert TimeSeries.query.filter_by(id=ts.id).count() == 0, "TimeSeries should be deleted."

    # Verify DataPoints are also deleted if cascade is set
    # Note: Ensure that cascade delete is configured in the relationship if expected
    assert DataPoint.query.filter_by(time_series_id=ts.id).count() == 0, "Associated DataPoints should be deleted with the TimeSeries."

def test_validate_code_len_decorator(app, create_seriesgroup_and_type):
    """
    Test that the validate_code_len decorator enforces code length constraints.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    # Attempt to create a SeriesGroup with a code exceeding CODE_MAX_LEN
    long_code = 'A' * 13  # Assuming CODE_MAX_LEN is 12

    with pytest.raises(ValueError) as exc_info:
        sg = SeriesGroup(name="LongCodeSG", description="SeriesGroup with long code", series_group_code=long_code)
        sg.save()

    db.session.rollback()
    assert "Code must be 12 characters or less." in str(exc_info.value), "Code length validation should fail for codes longer than 12 characters."

def test_seriesgroup_nested_relationship(app, create_seriesgroup_and_type):
    """
    Test creating nested SeriesGroups (grandchildren).
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    # Create child SeriesGroups
    child1 = SeriesGroup(name="ChildSG1", description="First Child SeriesGroup", series_group_code="CSG001", parent=seriesgroup)
    child2 = SeriesGroup(name="ChildSG2", description="Second Child SeriesGroup", series_group_code="CSG002", parent=seriesgroup)
    db.session.add_all([child1, child2])
    db.session.commit()

    # Create grandchildren
    grandchild1 = SeriesGroup(name="GrandChildSG1", description="First Grandchild SeriesGroup", series_group_code="GCSG001", parent=child1)
    grandchild2 = SeriesGroup(name="GrandChildSG2", description="Second Grandchild SeriesGroup", series_group_code="GCSG002", parent=child1)
    db.session.add_all([grandchild1, grandchild2])
    db.session.commit()

    # Verify relationships
    assert grandchild1.parent == child1, "Grandchild1's parent should be ChildSG1."
    assert grandchild2.parent == child1, "Grandchild2's parent should be ChildSG1."
    assert child1 in seriesgroup.children.all(), "ChildSG1 should be a child of ParentSG."
    assert grandchild1 in child1.children.all(), "GrandChildSG1 should be a child of ChildSG1."
    assert grandchild2 in child1.children.all(), "GrandChildSG2 should be a child of ChildSG1."

def test_seriesbase_keywords_relationship(app, create_seriesgroup_and_type):
    """
    Test that SeriesBase (SeriesGroup and TimeSeries) can have multiple keywords associated.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    # Add keywords to SeriesGroup
    seriesgroup.add_keyword("Finance")
    seriesgroup.add_keyword("Investment")
    db.session.commit()

    # Verify keywords
    assert len(seriesgroup.keywords) == 2, "SeriesGroup should have two keywords."
    keywords = [kw.word for kw in seriesgroup.keywords]
    assert "Finance" in keywords, "SeriesGroup should have the 'Finance' keyword."
    assert "Investment" in keywords, "SeriesGroup should have the 'Investment' keyword."

    # Create a TimeSeries and add keywords
    ts = TimeSeries(name="TS_Keywords", time_series_type=tstype, code="TSK001")
    ts.add_keyword("Growth")
    ts.add_keyword("Value")
    db.session.add(ts)
    db.session.commit()

    # Verify keywords
    assert len(ts.keywords) == 2, "TimeSeries should have two keywords."
    ts_keywords = [kw.word for kw in ts.keywords]
    assert "Growth" in ts_keywords, "TimeSeries should have the 'Growth' keyword."
    assert "Value" in ts_keywords, "TimeSeries should have the 'Value' keyword."

def test_datapoint_date_release_logic(app, create_seriesgroup_and_type):
    """
    Test the conditional assignment of date_release in DataPoint.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    # Create a TimeSeries
    ts = TimeSeries(name="TS_DateRelease", time_series_type=tstype, code="TSD001")
    db.session.add(ts)
    db.session.commit()

    # Create DataPoints with and without date_release
    dp1 = DataPoint(date=datetime.date(2025, 1, 1), value=100.0, date_release=datetime.date(2025, 1, 2), time_series=ts)
    dp2 = DataPoint(date=datetime.date(2025, 1, 3), value=110.0, time_series=ts)  # No date_release
    db.session.add_all([dp1, dp2])
    db.session.commit()

    # Verify date_release
    assert dp1.date_release == datetime.date(2025, 1, 2), "DataPoint1's date_release should be set correctly."
    assert dp2.date_release is None, "DataPoint2's date_release should be None."

def test_seriesbase_unique_constraints(app, create_seriesgroup_and_type):
    """
    Test unique constraints on SeriesGroup and TimeSeries.
    """
    seriesgroup, tstype = create_seriesgroup_and_type

    # Create a SeriesGroup with a unique code
    sg1 = SeriesGroup(name="SG_Unique1", description="Unique SeriesGroup 1", series_group_code="SGU001")
    db.session.add(sg1)
    db.session.commit()

    # Attempt to create another SeriesGroup with the same code
    sg2 = SeriesGroup(name="SG_Unique2", description="Unique SeriesGroup 2", series_group_code="SGU001")
    db.session.add(sg2)

    with pytest.raises(Exception) as exc_info:
        db.session.commit()

    db.session.rollback()
    assert 'UNIQUE constraint failed' in str(exc_info.value), "Duplicate SeriesGroup codes should violate unique constraint."

    # Similarly, test TimeSeries unique code
    ts1 = TimeSeries(name="TS_Unique1", time_series_type=tstype, code="TSU001")
    db.session.add(ts1)
    db.session.commit()

    ts2 = TimeSeries(name="TS_Unique2", time_series_type=tstype, code="TSU001")
    db.session.add(ts2)

    with pytest.raises(Exception) as exc_info_ts:
        db.session.commit()

    db.session.rollback()
    assert 'UNIQUE constraint failed' in str(exc_info_ts.value), "Duplicate TimeSeries codes should violate unique constraint."

================================================================================

FILE NAME: tests/test_save_models.py

# tests/test_save_models.py

import pytest
import datetime
from app import db
from app.models import SeriesGroup, TimeSeriesType, TimeSeries, DataPoint

def test_save_seriesgroup(app):
    """
    Test saving a standalone SeriesGroup to the database using the `save` method.
    """
    seriesgroup = SeriesGroup(name="SG1", description="Test SeriesGroup", code="SG001")
    seriesgroup.save()  # or db.session.add(seriesgroup); db.session.commit()

    assert seriesgroup.id is not None, "SeriesGroup ID should be generated after saving."
    assert SeriesGroup.query.count() == 1, "Exactly one SeriesGroup should exist in the database."

    retrieved = SeriesGroup.query.first()
    assert retrieved.name == "SG1", "Retrieved SeriesGroup name should match 'SG1'."
    assert retrieved.description == "Test SeriesGroup", "SeriesGroup description should match."
    assert retrieved.series_group_code == "SG001", "SeriesGroup series_group_code should match 'SG001'."

def test_save_time_series_type(app):
    """
    Test saving a TimeSeriesType to the database using the `save` method.
    """
    tst = TimeSeriesType(name="Price", description="Price Time Series")
    tst.save()

    assert tst.id is not None, "TimeSeriesType ID should be set after saving."
    assert TimeSeriesType.query.count() == 1, "Exactly one TimeSeriesType should exist."

    retrieved = TimeSeriesType.query.first()
    assert retrieved.name == "Price", "Retrieved name should be 'Price'."
    assert retrieved.description == "Price Time Series", "TimeSeriesType description mismatch."

def test_save_time_series_with_dependencies(app):
    """
    Test saving a TimeSeries that depends on a SeriesGroup and a TimeSeriesType.
    Verifies the parent objects can be saved together if they're new.
    """
    seriesgroup = SeriesGroup(name="SG2", description="Second SeriesGroup", series_group_code="SG002")
    tstype = TimeSeriesType(name="Volume", description="Volume Time Series")

    ts = TimeSeries(
        name="TS-Test",
        time_series_type=tstype,  # Correct: Assign the object, not the ID
        code="TST"
    )
    ts.series_groups.append(seriesgroup)

    ts.save()  # Should save ts, seriesgroup & tstype, too.

    # Verify TimeSeries
    assert ts.id is not None, "TimeSeries ID should be set after saving."
    assert TimeSeries.query.count() == 1, "One TimeSeries should exist."

    # Verify SeriesGroup
    assert seriesgroup.id is not None, "SeriesGroup ID should be set after saving TimeSeries."
    assert SeriesGroup.query.count() == 1, "One SeriesGroup should exist."

    # Verify TimeSeriesType
    assert tstype.id is not None, "TimeSeriesType ID should be set after saving TimeSeries."
    assert TimeSeriesType.query.count() == 1, "One TimeSeriesType should exist."
    
    # Additionally, check that the association is correct
    retrieved_ts = TimeSeries.query.first()
    associated_sgs = retrieved_ts.series_groups.all()
    assert len(associated_sgs) == 1, "TimeSeries should be associated with one SeriesGroup."
    assert associated_sgs[0].id == seriesgroup.id, "Associated SeriesGroup should match the created one."

def test_save_data_points_with_timeseries(app):
    """
    Test saving a TimeSeries along with multiple DataPoints.
    Ensures child DataPoints are also saved.
    """
    seriesgroup = SeriesGroup(name="SG3", description="Third SeriesGroup", series_group_code="SG003")
    tstype = TimeSeriesType(name="Price", description="Price Time Series")

    ts = TimeSeries(name="TS-DataPoints", time_series_type=tstype, code="109")  # Correct assignment
    ts.series_groups.append(seriesgroup)

    dp1 = DataPoint(date=datetime.date(2025, 1, 10), value=4000.0)
    dp2 = DataPoint(date=datetime.date(2025, 1, 11), value=4050.5)
    ts.data_points.extend([dp1, dp2])

    ts.save()  # Should save ts, seriesgroup, tstype, and dp1/dp2

    # Verify
    assert TimeSeries.query.count() == 1, "One TimeSeries should be saved."
    assert DataPoint.query.count() == 2, "Two DataPoints should be saved."

    retrieved_ts = TimeSeries.query.first()
    assert len(retrieved_ts.data_points) == 2, "Retrieved TimeSeries should have 2 DataPoints."
    
    # Verify SeriesGroup association
    associated_sgs = retrieved_ts.series_groups.all()
    assert len(associated_sgs) == 1, "TimeSeries should be associated with one SeriesGroup."
    assert associated_sgs[0].id == seriesgroup.id, "Associated SeriesGroup should match the created one."

def test_save_datapoint_alone_with_parents(app):
    """
    Test saving a single DataPoint that has a reference to a new TimeSeries,
    which references a new SeriesGroup and TimeSeriesType. All should be saved.
    """
    seriesgroup = SeriesGroup(name="SG4", description="Fourth SeriesGroup", series_group_code="SG004")
    tstype = TimeSeriesType(name="Bids", description="Bid Time Series")
    ts = TimeSeries(name="TS-Bids", time_series_type=tstype, code="1291")  # Correct assignment
    ts.series_groups.append(seriesgroup)

    dp = DataPoint(date=datetime.date(2025, 1, 12), value=5001.5, time_series=ts)
    dp.save()  # Should cascade and save dp, ts, seriesgroup, tstype

    assert dp.id is not None, "DataPoint should have an ID after saving."
    assert seriesgroup.id is not None, "SeriesGroup should be saved."
    assert tstype.id is not None, "TimeSeriesType should be saved."
    assert ts.id is not None, "TimeSeries should be saved."

    # Verify counts
    assert SeriesGroup.query.count() == 1, "Exactly one SeriesGroup should be in DB."
    assert TimeSeriesType.query.count() == 1, "Exactly one TimeSeriesType should be in DB."
    assert TimeSeries.query.count() == 1, "Exactly one TimeSeries should be in DB."
    assert DataPoint.query.count() == 1, "Exactly one DataPoint should be in DB."

    # Optionally, verify the association
    retrieved_ts = TimeSeries.query.first()
    associated_sgs = retrieved_ts.series_groups.all()
    assert len(associated_sgs) == 1, "TimeSeries should be associated with one SeriesGroup."
    assert associated_sgs[0].id == seriesgroup.id, "Associated SeriesGroup should match the created one."

def test_save_multiple_objects_in_one_session(app):
    """
    Test saving multiple objects in one transaction without committing until the end.
    """
    seriesgroup = SeriesGroup(name="SG5", description="Fifth SeriesGroup", series_group_code="SG005")
    tstype = TimeSeriesType(name="Spread", description="Spread Time Series")
    ts = TimeSeries(name="TS-Spread", time_series_type=tstype, code="183")  # Correct assignment
    ts.series_groups.append(seriesgroup)
    dp = DataPoint(date=datetime.date(2025, 1, 13), value=3999.9, time_series=ts)

    # Manually pass commit=False to gather them in the session, then commit once
    seriesgroup.save(commit=False)
    tstype.save(commit=False)
    ts.save(commit=False)
    dp.save(commit=False)

    # Now commit explicitly
    db.session.commit()

    # Verify
    assert SeriesGroup.query.count() == 1, "Exactly one SeriesGroup should be saved."
    assert TimeSeriesType.query.count() == 1, "Exactly one TimeSeriesType should be saved."
    assert TimeSeries.query.count() == 1, "Exactly one TimeSeries should be saved."
    assert DataPoint.query.count() == 1, "Exactly one DataPoint should be saved."

    retrieved_dp = DataPoint.query.first()
    assert retrieved_dp.value == 3999.9, "DataPoint value should be as assigned."
    assert retrieved_dp.time_series.name == "TS-Spread", "DataPoint should link to the correct TimeSeries."

================================================================================

FILE NAME: tests/fixtures.py

# tests/fixtures.py

import pytest
import random
import string
from app.models import SeriesGroup, TimeSeriesType, TimeSeries, DataPoint
import datetime
import pandas as pd
from app import db
import numpy as np


@pytest.fixture
def create_seriesgroup_and_type(app):
    """
    Fixture to create a SeriesGroup and a TimeSeriesType.
    Returns the objects themselves.
    """
    seriesgroup = SeriesGroup(name="SG_Test", description="Test SeriesGroup", series_group_code="SG999")
    tstype = TimeSeriesType(name="TestType", description="Test TimeSeriesType")
    db.session.add_all([seriesgroup, tstype])
    db.session.commit()
    return seriesgroup, tstype

@pytest.fixture
def populate_test_db(app):
    """
    Fixture to populate the test database with initial data.
    Accepts parameters for the number of series groups and data points per time series.
    """

    def _populate_test_db(num_seriesgroups=3, num_data_points=5, start_date=datetime.date(2024, 1, 1)):
        """
        Internal function to populate the test database.
        
        Args:
            num_seriesgroups (int): Number of SeriesGroups to create.
            num_data_points (int): Number of data points per time series.
            start_date (date or str): Start date for generating data points.
        """

        # Convert `start_date` to a `datetime.date` object if it's a string
        if isinstance(start_date, str):
            start_date = datetime.datetime.strptime(start_date, "%Y-%m-%d").date()

        def generate_random_name(length=3):
            """Generate a random name with the specified length."""
            return ''.join(random.choices(string.ascii_uppercase, k=length))

        def create_data_points(time_series_id, num_points, start_date):
            """Create random data points for a given time series."""
            data_points = []
            for i in range(num_points):
                date = start_date + datetime.timedelta(days=i)
                value = round(random.uniform(1000.0, 5000.0), 2)  # Random value between 1000 and 5000

                # Randomly decide whether to assign a `date_release` or leave it as None
                if random.choice([True, False]):
                    date_release = date + datetime.timedelta(days=1)
                else:
                    date_release = None

                data_point = DataPoint(
                    date=date,
                    value=value,
                    date_release=date_release,
                    time_series_id=time_series_id
                )
                data_points.append(data_point)
            return data_points

        with app.app_context():
            # Set a fixed random seed for reproducibility
            random.seed(42)

            # Create a TimeSeriesType
            time_series_type = TimeSeriesType(name='Price', description='Price Time Series')
            db.session.add(time_series_type)
            db.session.commit()

            # Generate the specified number of SeriesGroups and time series
            for _ in range(num_seriesgroups):
                # Create a random SeriesGroup
                seriesgroup_name = generate_random_name()
                series_group_code = ''.join(random.choices(string.ascii_uppercase + string.digits, k=5))
                seriesgroup = SeriesGroup(
                    name=seriesgroup_name,
                    description=f"{seriesgroup_name} Description",
                    series_group_code=series_group_code
                )
                db.session.add(seriesgroup)
                db.session.commit()

                # Create a random TimeSeries for the SeriesGroup
                time_series_name = generate_random_name()
                time_series = TimeSeries(
                    name=f"{seriesgroup_name} {time_series_name}",
                    type_id=time_series_type.id,
                    delta_type='pct',  # Assuming default or desired value
                    code=series_group_code + time_series_name[0].upper()
                )
                db.session.add(time_series)
                db.session.commit()

                # Associate TimeSeries with SeriesGroup
                seriesgroup.series.append(time_series)
                db.session.commit()

                # Create random data points for the time series
                data_points = create_data_points(time_series.id, num_data_points, start_date)
                db.session.add_all(data_points)
                db.session.commit()

    return _populate_test_db

@pytest.fixture
def sample_df_single_column():
    """
    Returns a single-column DataFrame (with a DateTimeIndex).
    """
    rng = np.random.default_rng(seed=42)
    returns = rng.normal(0.0001, 0.01, 252)
    prices = 100 * (1 + np.cumsum(returns))
    dates = pd.date_range("2025-01-01", periods=252, freq="D")
    return pd.DataFrame({"price": prices}, index=dates)

@pytest.fixture
def sample_df_multiple_columns():
    """
    Returns a multi-column DataFrame (with a DateTimeIndex).
    """
    rng = np.random.default_rng(seed=42)
    returns = rng.normal(0.0001, 0.01, 252)
    prices = 100 * (1 + np.cumsum(returns))
    volumes = rng.integers(1000, 10000, 252)
    dates = pd.date_range("2025-01-01", periods=252, freq="D")
    return pd.DataFrame({"price": prices, "volume": volumes}, index=dates)

================================================================================



================================================================================

FILE NAME: scripts/app.txt

App files:

================================================================================

FILE NAME: app/models.py

# app/models.py

from app import db
import pandas as pd
from sqlalchemy.sql import func
import datetime
import re

DELTA_TYPES = ['pct', 'abs']
DEFAULT_DELTA_TYPE = 'pct'
CODE_MAX_LEN = 12
TIME_FREQUENCIES = ['DA', 'D', 'W', 'M', 'B', 'Q', 'S', 'Y']


def validate_code_len(_validate_code):
    def wrapper(*args, **kwargs):
        code = _validate_code(*args, **kwargs)
        if isinstance(code, str):
            if len(code) > CODE_MAX_LEN:
                raise ValueError(f"Code must be {CODE_MAX_LEN} characters or less.")
        return code
    return wrapper

# Association table for many-to-many relationship between SeriesGroup and SeriesBase
seriesgroup_seriesbase = db.Table(
    'seriesgroup_seriesbase',
    db.Column('seriesgroup_id', db.Integer, db.ForeignKey('series_group.id'), primary_key=True),
    db.Column('seriesbase_id', db.Integer, db.ForeignKey('series_base.id'), primary_key=True)
)

# Association table for many-to-many relationship between SeriesBase and Keyword
seriesbase_keyword = db.Table(
    'seriesbase_keyword',
    db.Column('seriesbase_id', db.Integer, db.ForeignKey('series_base.id'), primary_key=True),
    db.Column('keyword_id', db.Integer, db.ForeignKey('keyword.id'), primary_key=True)
)

class BaseModel(db.Model):
    __abstract__ = True

    def save(self, session=None, commit=True):
        """
        Saves the current instance to the database, ensuring any dependent objects are also saved.
        session (db.session): The SQLAlchemy session to use. If None, uses db.session.
        commit (bool): Whether or not to commit immediately.
        """
        if session is None:
            session = db.session

        self._save_dependencies(session)

        session.add(self)

        if commit:
            session.commit()

        # Flush to ensure the instance is bound to the session
        session.flush()

        # Handle pending keywords if any
        if hasattr(self, '_pending_keywords'):
            for kw in self._pending_keywords:
                self.add_keyword(kw, session=session)
            del self._pending_keywords  # Clear pending keywords

    def _save_dependencies(self, session):
        """
        Override this in child classes to save any dependencies (parents or children).
        Default implementation does nothing.
        """
        pass

class Keyword(BaseModel):
    __tablename__ = 'keyword'
    id = db.Column(db.Integer, primary_key=True)
    word = db.Column(db.String(50), unique=True, nullable=False, index=True)

    # Relationship to SeriesBase
    series = db.relationship(
        'SeriesBase',
        secondary=seriesbase_keyword,
        back_populates='keywords',
        lazy='select'
    )

    def __repr__(self):
        return f'<Keyword {self.word}>'

class SeriesBase(BaseModel):
    __tablename__ = 'series_base'
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    description = db.Column(db.String(200))
    type = db.Column(db.String(50))  # Discriminator column

    date_create = db.Column(db.DateTime(timezone=True), server_default=func.now(), nullable=False)
    date_update = db.Column(
        db.DateTime(timezone=True), server_default=func.now(),
        onupdate=func.now(), nullable=False
    )

    keywords = db.relationship(
        'Keyword',
        secondary=seriesbase_keyword,
        back_populates='series',
        lazy='select'  # Changed from 'dynamic' to 'select'
    )

    def __init__(self, name, description=None, keywords=None, **kwargs):
        """
        Initializes a SeriesBase instance.

        Parameters:
        - name (str): The name of the series.
        - description (str, optional): A description of the series.
        - keywords (list of str, optional): A list of keyword strings to associate with the series.
        - **kwargs: Additional keyword arguments for other fields.
        """
        super().__init__(**kwargs)
        self.name = name
        self.description = description
        if keywords:
            self._pending_keywords = keywords.copy()  # Store pending keywords
        else:
            self._pending_keywords = []

    __mapper_args__ = {
        'polymorphic_identity': 'series_base',
        'polymorphic_on': type,
        'with_polymorphic': '*'
    }

    def __repr__(self):
        return f'<SeriesBase {self.name}>'

    def add_keyword(self, keyword_word, session=None):
        """
        Adds a keyword to the series. Creates the keyword if it doesn't exist.

        Parameters:
        - keyword_word (str): The keyword string to add.
        - session (Session, optional): The SQLAlchemy session to use. Defaults to db.session.
        """
        if not isinstance(keyword_word, str):
            raise TypeError("Keyword must be a string.")
        if len(keyword_word) > 50:
            raise ValueError("Keyword must be 50 characters or less.")

        if session is None:
            session = db.session

        if self not in session:
            session.add(self)

        keyword = session.query(Keyword).filter_by(word=keyword_word).first()
        if not keyword:
            keyword = Keyword(word=keyword_word)
            session.add(keyword)
        if keyword not in self.keywords:
            self.keywords.append(keyword)

    def remove_keyword(self, keyword_word):
        """
        Removes a keyword from the series.

        Parameters:
        - keyword_word (str): The keyword string to remove.
        """
        keyword = Keyword.query.filter_by(word=keyword_word).first()
        if keyword and keyword in self.keywords:
            self.keywords.remove(keyword)

    @staticmethod
    @validate_code_len
    def _validate_code(code, class_code):
        if class_code is not None and code is not None:
            if class_code == code:
                return class_code
            else:
                raise ValueError(
                    "Only Code or the code specified by the class must be passed. "
                    + "'code' is the same as the class code, but it aims to facilitate instantiation."
                )
        elif class_code is not None:
            return class_code
        elif code is not None:
            return code
        else:
            raise ValueError("Either code or class code ('time_series_code' or 'series_group_code') must be provided.") 
        
    @staticmethod
    def _validate_delta_type(delta_type):
        if delta_type is None:
            return DEFAULT_DELTA_TYPE
        if delta_type.lower() not in DELTA_TYPES:
            raise ValueError("delta_type must be one of the following: " + ", ".join(DELTA_TYPES))
        return delta_type.lower()

class SeriesGroup(SeriesBase):
    __tablename__ = 'series_group'
    id = db.Column(db.Integer, db.ForeignKey('series_base.id'), primary_key=True)
    series_group_code = db.Column(db.String(CODE_MAX_LEN), nullable=False, unique=True)  # Renamed and kept unique

    def __init__(self, name, description=None, series_group_code=None, code=None, keywords=None, **kwargs):
        """
        Initializes a SeriesGroup instance.

        Parameters:
        - name (str): The name of the series group.
        - description (str, optional): A description of the series group.
        - series_group_code (str): A unique code for the series group.
        - keywords (list of str, optional): A list of keyword strings to associate with the series group.
        - **kwargs: Additional keyword arguments for other fields.
        """
        super().__init__(name, description=description, keywords=keywords, **kwargs)
        self.series_group_code = self._validate_code(code, series_group_code)


    # Self-referential relationship for nested SeriesGroups
    parent_id = db.Column(db.Integer, db.ForeignKey('series_group.id'), nullable=True)
    children = db.relationship(
        'SeriesGroup',
        backref=db.backref('parent', remote_side=[id]),
        lazy='dynamic',
        foreign_keys=[parent_id]  # Explicitly specify foreign_keys
    )

    # Many-to-many relationship with SeriesBase (TimeSeries and SeriesGroup)
    series = db.relationship(
        'SeriesBase',
        secondary=seriesgroup_seriesbase,
        backref=db.backref('series_groups', lazy='dynamic'),
        lazy='dynamic'
    )

    __mapper_args__ = {
        'polymorphic_identity': 'series_group',
    }

    def __repr__(self):
        return (
            f'SeriesGroup(name={self.name}, code={self.time_series_code}, '
            + f'n_children={self.series.count()})'
        )

class TimeSeries(SeriesBase):
    __tablename__ = 'time_series'
    id = db.Column(db.Integer, db.ForeignKey('series_base.id'), primary_key=True)
    time_series_code = db.Column(db.String(CODE_MAX_LEN), nullable=False, unique=True)  # New unique field
    type_id = db.Column(db.Integer, db.ForeignKey('time_series_type.id'), nullable=True)
    time_frequency = db.Column(db.String(3), nullable=True, default='M')
    delta_type = db.Column(db.String(10), nullable=True, default='pct')

    def save(
            self,
            allow_update=True,
            keep_old_description=True,
            keep_old_delta_type=True,
            keep_old_time_frequency=False,
            join_keywords=True,
            join_data_points=True,
            session=None,
            commit=True
        ):
        # check if timeseries with that name and or code already exists
        time_series_with_same_name = TimeSeries.query.filter_by(name=self.name).first()
        time_series_with_same_code = TimeSeries.query.filter_by(time_series_code=self.time_series_code).first()

        if time_series_with_same_name or time_series_with_same_code:
            if not allow_update:
                if time_series_with_same_name and time_series_with_same_code:
                    raise ValueError("TimeSeries with the same name and code already exists.")
                elif time_series_with_same_name:
                    raise ValueError("TimeSeries with the same name already exists.")
                elif time_series_with_same_code:
                    raise ValueError("TimeSeries with the same code already exists.")
            else:
                if time_series_with_same_name and time_series_with_same_code:
                    if time_series_with_same_code != time_series_with_same_name:
                        raise ValueError("One time series with the same name and another with the same code already exists. Change one of them.")
                old_time_series = time_series_with_same_name or time_series_with_same_code
                self.id = old_time_series.id
                self.date_create = old_time_series.date_create
                if keep_old_description:
                    if old_time_series.description is not None:
                        self.description = old_time_series.description
                if keep_old_delta_type:
                    if old_time_series.delta_type is not None:
                        self.delta_type = old_time_series.delta_type
                if keep_old_time_frequency:
                    if old_time_series.time_frequency is not None:
                        self.time_frequency = old_time_series.time_frequency
                else:
                    if self.time_frequency is None:
                        self.time_frequency = old_time_series.time_frequency
                if join_keywords:
                    self.join_keywords(old_time_series.keywords)
                self.date_update = func.now()
                if join_data_points:
                    self.join_data_points(old_time_series.data_points)
                db.session.delete(old_time_series)

        super().save(session=session, commit=commit)


    def join_data_points(self, new_data_points):
        """
        Joins the provided DataPoint objects to the TimeSeries object.
        """
        self.data_points.extend(new_data_points)

    def join_keywords(self, new_keywords):
        """
        Joins the provided Keyword objects to the TimeSeries object.
        """
        self.keywords.extend(new_keywords)

    @property
    def number_data_points(self):
        try:
            return len(self.data_points)
        except TypeError:
            return 0

    def __repr__(self):
        return (
            f'TimeSeries(name={self.name}, code={self.time_series_code}, '
            + f'len={self.number_data_points}, freq={self.time_frequency}, delta={self.delta_type})'
        )

    data_points = db.relationship(
        'DataPoint',
        backref='time_series',
        lazy=True,
        cascade='all, delete-orphan'
    )

    __mapper_args__ = {
        'polymorphic_identity': 'time_series',
    }

    def __init__(self, name, code=None, time_series_code=None, type_id=None, time_frequency=None, delta_type=None, **kwargs):
        super().__init__(name, **kwargs)
        self.time_series_code = self._validate_code(code, time_series_code)
        self.type_id = type_id
        self.time_frequency = time_frequency
        self.delta_type = self._validate_delta_type(delta_type)


    def _save_dependencies(self, session):
        """
        Ensures the related TimeSeriesType and DataPoint objects are also saved.
        """
        # Save the parent TimeSeriesType if it's new or modified
        if self.time_series_type:
            session.add(self.time_series_type)

        # Save child DataPoints
        for dp in self.data_points:
            session.add(dp)

    def to_dataframe(
            self,
            only_most_recent_per_date=True,
            filter_date_release_smaller_or_equal_to=None,
            include_date_release=False,
            include_date_create=False
        ):
        """
        Returns the TimeSeries data as a pandas DataFrame.
        """
        data = {
            'date': [dp.date for dp in self.data_points],
            'value': [dp.value for dp in self.data_points],
            'date_create': [dp.date_create for dp in self.data_points],
            'date_release': [dp.date_release for dp in self.data_points]
        }
        ts_dataframe = (
            pd.DataFrame(data)
            .sort_values(['date', 'date_release', 'date_create'])
        )
        if only_most_recent_per_date:
            ts_dataframe = ts_dataframe.drop_duplicates(subset=['date'])
        ts_dataframe = (
            ts_dataframe
            .set_index('date')
            .rename({'value': self.name}, axis=1)
        )
        if filter_date_release_smaller_or_equal_to is not None:
            if isinstance(filter_date_release_smaller_or_equal_to, str):
                try:
                    filter_date = pd.to_datetime(filter_date_release_smaller_or_equal_to)
                except ValueError:
                    raise ValueError("filter_date_release_smaller_or_equal_to must be a valid date string.")
            elif not isinstance(filter_date_release_smaller_or_equal_to, (datetime.datetime, datetime.date)):
                raise ValueError("filter_date_release_smaller_or_equal_to must be a valid date string.")
            else:
                filter_date = filter_date_release_smaller_or_equal_to
            ts_dataframe = ts_dataframe[ts_dataframe['date_release'] <= filter_date]
        
        if not include_date_release:
            ts_dataframe = ts_dataframe.drop(columns=['date_release'])
        if not include_date_create:
            ts_dataframe = ts_dataframe.drop(columns=['date_create'])
        return ts_dataframe

    @classmethod
    def from_dataframe(
        cls,
        df,
        series_groups=None,
        time_series_type=None,
        name=None,
        code=None,
        time_frequency=None,
        description=None,
        date_column=None,
        all_columns_have_same_series_groups=False
    ):
        """
        Creates one or more TimeSeries objects from a pandas DataFrame without saving to the database.

        Returns
        -------
        TimeSeries or List[TimeSeries]
            A single TimeSeries if exactly one column is processed, or a list of multiple TimeSeries objects.
        """
        if date_column is not None:
            if date_column not in df.columns:
                raise ValueError(f"Date column '{date_column}' not found in DataFrame.")
            df = df.set_index(date_column)

        if 'date' in [c.lower() for c in df.columns.tolist()]:
            df = df.set_index('date')
        try:
            df.index = pd.to_datetime(df.index)
        except ValueError:
            raise ValueError("The DataFrame must have a datetime index or specify a date_column.")
        df.index.name = 'date'

        if len(df.columns) == 1:
            if name is None:
                name = df.columns[0]

            if isinstance(time_frequency, (list, tuple)):
                if len(time_frequency) != 1:
                    raise ValueError("Time frequency list must match the number of columns in the DataFrame.")
                time_frequency = time_frequency[0]
            if isinstance(time_frequency, str):
                if time_frequency not in TIME_FREQUENCIES:
                    raise ValueError("Time frequency must be one of the following: " + ", ".join(TIME_FREQUENCIES))
                
            if code is None:
                raise ValueError("Code must be provided for each column passed.")
            elif isinstance(code, (list, tuple)):
                if len(code) != 1:
                    raise ValueError("Code list must match the number of columns in the DataFrame.")
                code = code[0]
            elif not isinstance(code, str):
                raise ValueError("Code must be provided as string or as a list/tuple with len=1.")

            if isinstance(time_series_type, (list, tuple)):
                if len(time_series_type) != 1:
                    raise ValueError("TimeSeriesType list must match the number of columns in the DataFrame.")
                time_series_type = time_series_type[0]
            if not isinstance(time_series_type, (str, TimeSeriesType)) and time_series_type is not None:
                raise ValueError("TimeSeriesType must be provided as string, None or a list/tuple with len=1.")

            return cls.build_time_series_object(
                df.iloc[:, 0].values,
                df.index,
                name,
                code,
                time_frequency,
                series_groups,  # Can be None
                TimeSeriesType._convert_to_time_series_type(time_series_type),
                description
            )
        else:
            if name is None:
                name = df.columns
            elif name and not isinstance(name, list):
                raise ValueError("Name must be a list if multiple columns are provided.")
            elif len(name) != len(df.columns):
                raise ValueError("Name list must match the number of columns in the DataFrame.")
            else:
                raise ValueError("Name must be provided as list")
            
            if isinstance(time_frequency, (list, tuple)):
                if len(time_frequency) != len(df.columns):
                    raise ValueError("Time frequency list must match the number of columns in the DataFrame or be equal for all.")
                if not all([tf in TIME_FREQUENCIES for tf in time_frequency]):
                    raise ValueError("Time frequency must be one of the following: " + ", ".join(TIME_FREQUENCIES))
            elif isinstance(time_frequency, str) or time_frequency is None:
                if time_frequency not in TIME_FREQUENCIES and time_frequency is not None:
                    raise ValueError("Time frequency must be one of the following: " + ", ".join(TIME_FREQUENCIES))
                time_frequency = [time_frequency] * len(df.columns)
            elif time_frequency is not None:
                raise ValueError("Time frequency must be a string or a list of strings.")
            
            if isinstance(time_series_type, (list, tuple)):
                if len(time_series_type) != len(df.columns):
                    raise ValueError("TimeSeriesType list must match the number of columns in the DataFrame.")
                if not all([isinstance(tst, (TimeSeriesType, str)) or tst is None for tst in time_series_type]):
                    raise ValueError("TimeSeriesType must be an instance of TimeSeriesType or string.")
            elif isinstance(time_series_type, (TimeSeriesType, str)) or time_series_type is None:
                time_series_type = [time_series_type] * len(df.columns)
            elif time_series_type is not None:
                raise ValueError("TimeSeriesType must be an instance of TimeSeriesType, string, or a list of strings.")
            
            if code is None or not isinstance(code, (list, tuple)):
                raise ValueError("Code must be provided for each column as a list or tuple passed.")
            elif len(code) != len(df.columns):
                raise ValueError("Code list must match the number of columns in the DataFrame.")
            
            # Allow series_groups to be optional
            if series_groups is not None:
                if isinstance(series_groups, (str, int, SeriesGroup)):
                    series_groups = [series_groups] * len(df.columns)
                elif not isinstance(series_groups, list):
                    raise ValueError("SeriesGroups must be provided as list, string, or SeriesGroup instances")
                    
                if len(series_groups) != len(df.columns):
                    if not all_columns_have_same_series_groups:
                        raise ValueError(
                            "SeriesGroups list must match the number of columns in the DataFrame or "
                            + "parameter 'all_columns_have_same_series_groups=True'."
                        )
            else:
                series_groups = [None] * len(df.columns)  # No groups associated

            if isinstance(description, str) and description is not None:
                raise ValueError("Description must be a list if multiple columns are provided.")
            
            if not all([g is None for g in series_groups]) and all_columns_have_same_series_groups:
                all_columns_have_same_series_groups = True
                col_series_groups = series_groups
            else:
                all_columns_have_same_series_groups = False


            all_columns_have_same_series_groups = (
                False if all([g is None for g in series_groups]) else all_columns_have_same_series_groups
            )

            all_time_series = []
            for i, col in enumerate(df.columns):
                if not all_columns_have_same_series_groups:
                    col_series_groups = series_groups[i]
                all_time_series.append(
                    cls.build_time_series_object(
                    df[col].values,
                    df.index,
                    name[i],
                    code[i],
                    time_frequency[i],
                    col_series_groups,
                    TimeSeriesType._convert_to_time_series_type(time_series_type[i]),
                    description
                ))
            return all_time_series

    @classmethod
    def save_from_dataframe(cls,
        df,
        series_groups=None,
        time_series_type=None,
        name=None,
        description=None,
        date_column=None,
        value_columns=None,
    ):
        """
        Creates one or more TimeSeries objects from a pandas DataFrame and saves them to the database.

        Returns
        -------
        TimeSeries or List[TimeSeries]
            A single TimeSeries if exactly one column is processed, or a list of multiple TimeSeries objects.
        """
        time_series_objects = cls.from_dataframe(
            df,
            series_groups,
            time_series_type,
            name,
            description,
            date_column,
            value_columns
        )
        if isinstance(time_series_objects, list):
            for ts in time_series_objects:
                ts.save()
        else:
            time_series_objects.save()
        return True

    @classmethod
    def build_time_series_object(cls, values, dates, time_series_name, time_series_code, time_frequency, series_groups, time_series_type, description=None):
        """
        Build a TimeSeries object with DataPoint objects from provided values and dates.
        """
        data_points = []
        for i in range(len(values)):
            data_points.append(DataPoint(date=dates[i], value=values[i]))
        ts = cls(
            name=time_series_name,
            code=time_series_code,
            data_points=data_points,
        )
        if time_frequency is not None:
            ts.time_frequency = time_frequency
        if description is not None:
            ts.description = description
        if isinstance(time_series_type, TimeSeriesType):
            ts.time_series_type = time_series_type
        else:
            ts.type_id = time_series_type

        # Associate with SeriesGroups if provided
        if series_groups:
            if isinstance(series_groups, SeriesGroup):
                ts.series_groups.append(series_groups)
            elif isinstance(series_groups, list):
                for sg in series_groups:
                    if sg is not None:
                        ts.series_groups.append(sg)
            else:
                raise ValueError("series_groups must be a SeriesGroup instance, list of SeriesGroup instances, or None")

        return ts

class DataPoint(BaseModel):
    __tablename__ = 'data_point'
    id = db.Column(db.Integer, primary_key=True)
    date = db.Column(db.Date, nullable=False)
    value = db.Column(db.Float, nullable=False)
    date_release = db.Column(db.Date, nullable=True)
    time_series_id = db.Column(db.Integer, db.ForeignKey('time_series.id'), nullable=False)

    date_create = db.Column(db.DateTime(timezone=True), server_default=func.now(), nullable=False)
    date_update = db.Column(
        db.DateTime(timezone=True), server_default=func.now(),
        onupdate=func.now(), nullable=False
    )

    def __repr__(self):
        return f'DP({self.date}: {self.value})'

    def _save_dependencies(self, session):
        """
        Ensure the parent TimeSeries (and potentially its parent objects) are saved.
        """
        if self.time_series:
            # Make sure the parent TimeSeries saves its dependencies too.
            # This will also add the SeriesGroups and any other DataPoints.
            self.time_series._save_dependencies(session)
            session.add(self.time_series)

class TimeSeriesType(BaseModel):
    __tablename__ = 'time_series_type'
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(50), nullable=False, unique=True)
    description = db.Column(db.String(400))
    time_series = db.relationship('TimeSeries', backref='time_series_type', lazy=True)

    @classmethod
    def _convert_to_time_series_type(cls, tst):
        if tst is None:
            return tst
        elif isinstance(tst, cls):
            return tst
        elif isinstance(tst, str):
            tst = cls.query.filter_by(name=tst).first()
            if tst is not None:
                return tst
            else:
                return cls(name=tst)
        else:
            raise ValueError("TimeSeriesType must be an instance of TimeSeriesType or a string.")
        

    date_create = db.Column(
        db.DateTime(timezone=True), server_default=func.now(),
        nullable=False
    )
    date_update = db.Column(
        db.DateTime(timezone=True), server_default=func.now(),
        onupdate=func.now(), nullable=False
    )

    __mapper_args__ = {
        'polymorphic_identity': 'time_series_type',
    }

    def __repr__(self):
        return f'TimeSeriesType(name={self.name})>'

================================================================================

FILE NAME: app/__init__.py

from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from config import config  # Import the config dictionary
from flask_migrate import Migrate
import os

db = SQLAlchemy()
migrate = Migrate()

def create_app(config_name=None):
    app = Flask(__name__)
    
    # Determine the configuration to use
    if config_name is None:
        config_name = os.getenv('FLASK_ENV') or 'default'
    
    # Use the configuration class from the config dictionary
    app.config.from_object(config[config_name])
    
    # Initialize extensions
    db.init_app(app)
    migrate.init_app(app, db)
    
    # Import and register Blueprints
    from .routes import main
    app.register_blueprint(main)
    
    with app.app_context():
        from . import models
    
    return app

================================================================================

FILE NAME: app/utils.py

import numpy as np
import pandas as pd
from sklearn.datasets import make_sparse_spd_matrix
from typing import Union
from typing_extensions import Literal

def create_returns_df(
        n_samples: int = 1000,
        n_assets: int = 5,
        avg_return: float = .004,
        alpha_sparsity: float = .3,
        seed: int = 42,
        end_date: str = "2024-01-01",
        date_frequecy: Union[Literal["ME", "BM", "BQ", "BA", "W", "D"]] = "ME", # For month
        variance_multiplier: float = .03,
        truncate: bool = True
    ) -> pd.DataFrame:
    if variance_multiplier > 0.5 or variance_multiplier <= 0:
        raise ValueError("variance_multiplier must be between 0 and 0.5")
    rng = np.random.RandomState(seed)
    asset_names = ["".join(rng.choice(list("ABCDEFGHIJKLMNOPQRSTUVWXYZ"), 3)) for i in range(n_assets)]
    cov_matrix = make_sparse_spd_matrix(n_dim=n_assets, alpha=alpha_sparsity)
    cov_matrix /= (np.max(cov_matrix) / variance_multiplier)
    returns = np.random.multivariate_normal(np.ones(n_assets) * avg_return, cov_matrix, n_samples)
    if truncate:
        returns[returns < -1] = -.95
    returns_df = pd.DataFrame(returns, columns=asset_names)
    returns_df.index = pd.date_range(end=end_date, periods=n_samples, freq=date_frequecy)
    return returns_df

================================================================================

FILE NAME: app/routes.py

from flask import Blueprint

# Define the Blueprint
main = Blueprint('main', __name__)

@main.route('/')
def home():
    return "Welcome to the Investment Portfolio App!"

================================================================================



================================================================================

