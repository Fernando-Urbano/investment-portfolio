The following contains a tree of the directory and all the important files of the current version of my project: 

================================================================================

FILE NAME: scripts/tree_output.txt

.
├── README.md
├── __pycache__
│   ├── config.cpython-312.pyc
│   └── run.cpython-312.pyc
├── app
│   ├── __init__.py
│   ├── __pycache__
│   │   ├── __init__.cpython-312.pyc
│   │   ├── models.cpython-312.pyc
│   │   └── routes.cpython-312.pyc
│   ├── models.py
│   ├── routes.py
│   ├── static
│   └── templates
├── config.py
├── join_files.py
├── requirements.txt
├── run.py
├── scripts
└── tests
    ├── __init__.py
    ├── __pycache__
    │   ├── __init__.cpython-312.pyc
    │   ├── conftest.cpython-312-pytest-8.3.4.pyc
    │   ├── fixtures.cpython-312.pyc
    │   ├── test_initial_database_population.cpython-312-pytest-8.3.4.pyc
    │   ├── test_save_modules.cpython-312-pytest-8.3.4.pyc
    │   └── test_timeseries.cpython-312-pytest-8.3.4.pyc
    ├── conftest.py
    ├── fixtures.py
    ├── test_initial_database_population.py
    ├── test_save_modules.py
    └── test_timeseries.py

9 directories, 25 files


================================================================================

FILE NAME: scripts/migrations.txt

Migrations files:

================================================================================



================================================================================

FILE NAME: scripts/tests.txt

Test files:

================================================================================

FILE NAME: tests/conftest.py

# tests/conftest.py

import pytest
import sys
import pandas as pd
import os

# Add the project root to sys.path
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from app import create_app, db


@pytest.fixture(scope='function')
def app():
    """
    Create and configure a new app instance for each test.
    """
    # Set the environment variable to 'testing'
    os.environ['FLASK_ENV'] = 'testing'
    
    # Create the Flask app with testing configuration
    app = create_app('testing')
    
    # Establish an application context
    with app.app_context():
        # Create the database and the database tables
        db.create_all()
        yield app
        # Drop the database tables after the test
        db.session.remove()
        db.drop_all()

@pytest.fixture(scope='function')
def client(app):
    """
    Create a test client for the app.
    """
    return app.test_client()

@pytest.fixture(scope='function')
def runner(app):
    """
    Create a CLI runner for the app.
    """
    return app.test_cli_runner()

# Import fixtures from fixtures.py
from .fixtures import (
    populate_test_db,
    sample_df_single_column,    
    sample_df_multiple_columns,
    create_asset_and_type  
)

================================================================================

FILE NAME: tests/test_timeseries.py

import pytest
import datetime
import pandas as pd
from app.models import TimeSeries, DataPoint, Asset, TimeSeriesType
from app import db  # Import db for database operations


def test_from_dataframe_single_column(
    app,
    sample_df_single_column,
    create_asset_and_type
):
    """
    Test that from_dataframe handles a single-column DataFrame and returns one TimeSeries.
    """
    asset_id, tstype_id = create_asset_and_type

    with app.app_context():
        # Re-query Asset and TimeSeriesType to ensure they're attached to the current session
        asset = db.session.get(Asset, asset_id)
        tstype = db.session.get(TimeSeriesType, tstype_id)

        ts_obj = TimeSeries.from_dataframe(
            df=sample_df_single_column,
            asset=asset,
            time_series_type=tstype
        )

        # Expect a single TimeSeries (not a list)
        assert isinstance(ts_obj, TimeSeries), "Should return a single TimeSeries object."
        assert ts_obj.name == "price", "TimeSeries name should match the DataFrame column name."
        assert len(ts_obj.data_points) == 252, "TimeSeries should have 252 DataPoints."

        # Check the data points
        for i, dp in enumerate(ts_obj.data_points):
            expected_date = sample_df_single_column.index[i].date()
            expected_value = sample_df_single_column["price"].iloc[i]
            assert dp.date.date() == expected_date, f"Expected date {expected_date}, got {dp.date}"
            assert dp.value == expected_value, f"Expected value {expected_value}, got {dp.value}"


def test_from_dataframe_single_column_with_asset_id_and_tstype_id(
    app,
    sample_df_single_column,
    create_asset_and_type
):
    """
    Test that from_dataframe handles a single-column DataFrame and returns one TimeSeries.
    """
    asset_id, tstype_id = create_asset_and_type

    with app.app_context():
        ts_obj = TimeSeries.from_dataframe(
            df=sample_df_single_column,
            asset=asset_id,
            time_series_type=tstype_id
        )

        # Expect a single TimeSeries (not a list)
        assert isinstance(ts_obj, TimeSeries), "Should return a single TimeSeries object."
        assert ts_obj.name == "price", "TimeSeries name should match the DataFrame column name."
        assert len(ts_obj.data_points) == 252, "TimeSeries should have 252 DataPoints."

        # Check the data points
        for i, dp in enumerate(ts_obj.data_points):
            expected_date = sample_df_single_column.index[i].date()
            expected_value = sample_df_single_column["price"].iloc[i]
            assert dp.date.date() == expected_date, f"Expected date {expected_date}, got {dp.date}"
            assert dp.value == expected_value, f"Expected value {expected_value}, got {dp.value}"


def test_from_dataframe_single_column_with_date_as_column(
    app,
    sample_df_single_column,
    create_asset_and_type
):
    """
    Test that from_dataframe handles a single-column DataFrame and returns one TimeSeries.
    """
    asset_id, tstype_id = create_asset_and_type

    sample_df = sample_df_single_column
    sample_df["Datetime"] = sample_df.index
    sample_df = sample_df.reset_index(drop=True)

    with app.app_context():
        ts_obj = TimeSeries.from_dataframe(
            df=sample_df_single_column,
            asset=asset_id,
            time_series_type=tstype_id,
            date_column="Datetime"
        )

        # Expect a single TimeSeries (not a list)
        assert isinstance(ts_obj, TimeSeries), "Should return a single TimeSeries object."
        assert ts_obj.name == "price", "TimeSeries name should match the DataFrame column name."
        assert len(ts_obj.data_points) == 252, "TimeSeries should have 252 DataPoints."

        # Check the data points
        for i, dp in enumerate(ts_obj.data_points):
            expected_date = sample_df_single_column.index[i].date()
            expected_value = sample_df_single_column["price"].iloc[i]
            assert dp.date.date() == expected_date, f"Expected date {expected_date}, got {dp.date}"
            assert dp.value == expected_value, f"Expected value {expected_value}, got {dp.value}"


def test_from_dataframe_multi_column(
    app,
    sample_df_multiple_columns,
    create_asset_and_type
):
    """
    Test that from_dataframe handles a multi-column DataFrame and returns multiple TimeSeries.
    """
    asset_id, tstype_id = create_asset_and_type
    num_columns = len(sample_df_multiple_columns.columns)

    with app.app_context():
        ts_objs = TimeSeries.from_dataframe(
            df=sample_df_multiple_columns,
            asset=asset_id,
            time_series_type=tstype_id
        )

        # Expect a list of TimeSeries objects
        assert isinstance(ts_objs, list), "Should return a list of TimeSeries objects."
        assert len(ts_objs) == num_columns, "Should return one TimeSeries per column."
        assert all(isinstance(ts, TimeSeries) for ts in ts_objs), "All elements should be TimeSeries objects."
        assert ts_objs[0].name == "price", "First TimeSeries name should match the first DataFrame column name."


def test_to_dataframe_multi_column(
    app,
    sample_df_multiple_columns,
    create_asset_and_type
):
    """
    Test that from_dataframe handles a multi-column DataFrame and returns multiple TimeSeries.
    """
    asset_id, tstype_id = create_asset_and_type

    with app.app_context():
        ts_objs = TimeSeries.from_dataframe(
            df=sample_df_multiple_columns,
            asset=asset_id,
            time_series_type=tstype_id
        )
        for ts_obj in ts_objs:
            ts_dataframe = ts_obj.to_dataframe()
            assert isinstance(ts_dataframe, pd.DataFrame), "Should return a DataFrame."
            assert len(ts_dataframe.index) == len(sample_df_multiple_columns.index), "DataFrame index length should match input length."


def test_to_dataframe_single_column(
    app,
    sample_df_single_column,
    create_asset_and_type
):
    """
    Test that from_dataframe handles a single-column DataFrame and returns one TimeSeries.
    """
    asset_id, tstype_id = create_asset_and_type

    with app.app_context():
        ts_obj = TimeSeries.from_dataframe(
            df=sample_df_single_column,
            asset=asset_id,
            time_series_type=tstype_id
        )

        ts_dataframe = ts_obj.to_dataframe()
        assert isinstance(ts_dataframe, pd.DataFrame), "Should return a DataFrame."
        assert len(ts_dataframe.index) == len(sample_df_single_column.index), "DataFrame index length should match input length."
        assert list(ts_dataframe.columns) == ["price"], "DataFrame columns should match the input DataFrame."
        for i in range(5):
            assert ts_dataframe.iloc[i, 0] == sample_df_single_column.iloc[i, 0], "DataFrame values should match the input DataFrame."


    

================================================================================

FILE NAME: tests/test_initial_database_population.py

# tests/test_initial_population.py

from app.models import Asset, TimeSeriesType, TimeSeries, DataPoint
import datetime

def test_initial_population(populate_test_db):
    """
    Test the initial population of the test database by verifying the created entries.
    """
    # Populate the database with 3 assets and 5 data points per time series
    populate_test_db(num_assets=3, num_data_points=5, start_date="2024-01-01")

    # Verify the number of entries created
    assert Asset.query.count() == 3, "There should be exactly 3 Assets."
    assert TimeSeriesType.query.count() == 1, "There should be exactly one TimeSeriesType."
    assert TimeSeries.query.count() == 3, "Each Asset should have exactly one TimeSeries."
    assert DataPoint.query.count() == 15, "Each TimeSeries should have 5 DataPoints."

    # Retrieve and check the Assets
    for asset in Asset.query.all():
        assert len(asset.name) == 3, f"Asset name '{asset.name}' should have exactly three characters."
        assert asset.description.endswith("Description"), f"Asset description mismatch for '{asset.name}'."
        assert isinstance(asset.is_tradable, bool), f"Asset '{asset.name}' should have a boolean value for 'is_tradable'."

    # Retrieve and check the TimeSeriesType
    time_series_type = TimeSeriesType.query.first()
    assert time_series_type.name == 'Price', "TimeSeriesType name should be 'Price'."
    assert time_series_type.description == 'Price Time Series', "TimeSeriesType description mismatch."

    # Retrieve and check the TimeSeries and their DataPoints
    for time_series in TimeSeries.query.all():
        assert len(time_series.name) > 0, "TimeSeries name should not be empty."
        assert time_series.type_id == time_series_type.id, "TimeSeries type_id mismatch."
        assert time_series.asset_id in [asset.id for asset in Asset.query.all()], "TimeSeries asset_id mismatch."

        data_points = time_series.data_points
        assert len(data_points) == 5, f"TimeSeries '{time_series.name}' should have exactly 5 DataPoints."

        # Verify the DataPoints
        start_date = datetime.date(2024, 1, 1)
        for i, dp in enumerate(data_points):
            expected_date = start_date + datetime.timedelta(days=i)
            assert dp.date == expected_date, f"DataPoint date mismatch: expected {expected_date}, got {dp.date}."
            assert isinstance(dp.value, float), "DataPoint 'value' should be a float."
            assert dp.time_series_id == time_series.id, f"DataPoint time_series_id mismatch for {dp}."

            # Check `date_release` conditionally
            if dp.date_release:
                assert dp.date_release == dp.date + datetime.timedelta(days=1), (
                    f"DataPoint 'date_release' mismatch: expected {dp.date + datetime.timedelta(days=1)}, got {dp.date_release}."
                )

================================================================================

FILE NAME: tests/__init__.py



================================================================================

FILE NAME: tests/test_save_modules.py

import pytest
import datetime
from app import db
from app.models import Asset, TimeSeriesType, TimeSeries, DataPoint

def test_save_asset(app):
    """
    Test saving a standalone Asset to the database using the `save` method.
    """
    asset = Asset(name="AST", description="Test Asset", is_tradable=True)
    asset.save()  # or db.session.add(asset); db.session.commit()

    assert asset.id is not None, "Asset ID should be generated after saving."
    assert Asset.query.count() == 1, "Exactly one Asset should exist in the database."

    retrieved = Asset.query.first()
    assert retrieved.name == "AST", "Retrieved Asset name should match 'AST'."
    assert retrieved.is_tradable is True, "Asset should be marked as tradable."

def test_save_time_series_type(app):
    """
    Test saving a TimeSeriesType to the database using the `save` method.
    """
    tst = TimeSeriesType(name="Price", description="Price Time Series")
    tst.save()

    assert tst.id is not None, "TimeSeriesType ID should be set after saving."
    assert TimeSeriesType.query.count() == 1, "Exactly one TimeSeriesType should exist."

    retrieved = TimeSeriesType.query.first()
    assert retrieved.name == "Price", "Retrieved name should be 'Price'."

def test_save_time_series_with_dependencies(app):
    """
    Test saving a TimeSeries that depends on an Asset and a TimeSeriesType.
    Verifies the parent objects can be saved together if they're new.
    """
    asset = Asset(name="AST2", description="Second Asset", is_tradable=False)
    tstype = TimeSeriesType(name="Volume", description="Volume Time Series")

    ts = TimeSeries(
        name="TS-Test",
        asset=asset,
        time_series_type=tstype
    )
    # If you have a custom save method that cascades, this will save asset & tstype, too.
    ts.save()

    # Verify TimeSeries
    assert ts.id is not None, "TimeSeries ID should be set after saving."
    assert TimeSeries.query.count() == 1, "One TimeSeries should exist."

    # Verify Asset
    assert asset.id is not None, "Asset ID should be set after saving TimeSeries."
    assert Asset.query.count() == 1, "One Asset should exist."

    # Verify TimeSeriesType
    assert tstype.id is not None, "TimeSeriesType ID should be set after saving TimeSeries."
    assert TimeSeriesType.query.count() == 1, "One TimeSeriesType should exist."

def test_save_data_points_with_timeseries(app):
    """
    Test saving a TimeSeries along with multiple DataPoints.
    Ensures child DataPoints are also saved.
    """
    asset = Asset(name="AST3", description="Third Asset", is_tradable=True)
    tstype = TimeSeriesType(name="Price", description="Price Time Series")

    ts = TimeSeries(name="TS-DataPoints", asset=asset, time_series_type=tstype)

    dp1 = DataPoint(date=datetime.date(2025, 1, 10), value=4000.0)
    dp2 = DataPoint(date=datetime.date(2025, 1, 11), value=4050.5)
    ts.data_points.extend([dp1, dp2])

    ts.save()  # Should save ts, asset, tstype, and dp1/dp2

    # Verify
    assert TimeSeries.query.count() == 1, "One TimeSeries should be saved."
    assert DataPoint.query.count() == 2, "Two DataPoints should be saved."

    retrieved_ts = TimeSeries.query.first()
    assert len(retrieved_ts.data_points) == 2, "Retrieved TimeSeries should have 2 DataPoints."

def test_save_datapoint_alone_with_parents(app):
    """
    Test saving a single DataPoint that has a reference to a new TimeSeries,
    which references a new Asset and TimeSeriesType. All should be saved.
    """
    asset = Asset(name="AST4", description="Fourth Asset")
    tstype = TimeSeriesType(name="Bids", description="Bid Time Series")
    ts = TimeSeries(name="TS-Bids", asset=asset, time_series_type=tstype)

    dp = DataPoint(date=datetime.date(2025, 1, 12), value=5001.5, time_series=ts)
    dp.save()  # Should cascade and save dp, ts, asset, tstype

    assert dp.id is not None, "DataPoint should have an ID after saving."
    assert asset.id is not None, "Asset should be saved."
    assert tstype.id is not None, "TimeSeriesType should be saved."
    assert ts.id is not None, "TimeSeries should be saved."

    # Verify counts
    assert Asset.query.count() == 1, "Exactly one Asset should be in DB."
    assert TimeSeriesType.query.count() == 1, "Exactly one TimeSeriesType should be in DB."
    assert TimeSeries.query.count() == 1, "Exactly one TimeSeries should be in DB."
    assert DataPoint.query.count() == 1, "Exactly one DataPoint should be in DB."

def test_save_multiple_objects_in_one_session(app):
    """
    Test saving multiple objects in one transaction without committing until the end.
    """
    asset = Asset(name="AST5", description="Fifth Asset", is_tradable=False)
    tstype = TimeSeriesType(name="Spread", description="Spread Time Series")
    ts = TimeSeries(name="TS-Spread", asset=asset, time_series_type=tstype)
    dp = DataPoint(date=datetime.date(2025, 1, 13), value=3999.9, time_series=ts)

    # Manually pass commit=False to gather them in the session, then commit once
    asset.save(commit=False)
    tstype.save(commit=False)
    ts.save(commit=False)
    dp.save(commit=False)

    # Now commit explicitly
    db.session.commit()

    # Verify
    assert Asset.query.count() == 1, "Exactly one Asset should be saved."
    assert TimeSeriesType.query.count() == 1, "Exactly one TimeSeriesType should be saved."
    assert TimeSeries.query.count() == 1, "Exactly one TimeSeries should be saved."
    assert DataPoint.query.count() == 1, "Exactly one DataPoint should be saved."

    retrieved_dp = DataPoint.query.first()
    assert retrieved_dp.value == 3999.9, "DataPoint value should be as assigned."
    assert retrieved_dp.time_series.name == "TS-Spread", "DataPoint should link to the correct TimeSeries."


# tests/test_exception_cases.py

import pytest
from sqlalchemy.exc import IntegrityError
from app import db
from app.models import TimeSeriesType, TimeSeries, Asset, DataPoint
import pandas as pd

def test_timeseries_type_duplicate_name_raises(app):
    """
    Verify that creating two TimeSeriesType objects with the same name
    (where name is unique) raises an IntegrityError.
    """
    # First TimeSeriesType
    tstype1 = TimeSeriesType(name="UniqueName", description="First Type")
    db.session.add(tstype1)
    db.session.commit()

    # Second TimeSeriesType with the same name
    tstype2 = TimeSeriesType(name="UniqueName", description="Second Type")
    db.session.add(tstype2)

    # Expect an IntegrityError upon commit
    with pytest.raises(IntegrityError):
        db.session.commit()

def test_timeseries_invalid_dataframe_raises(app):
    """
    Verify that TimeSeries.from_dataframe raises ValueError
    if the DataFrame doesn't have a proper date column/index.
    """
    # Create a minimal DataFrame that lacks a datetime column/index
    df = pd.DataFrame({
        "some_column": [1, 2, 3]
    })

    # Create required objects
    asset = Asset(name="AST", description="Asset for test", is_tradable=True)
    db.session.add(asset)
    db.session.commit()

    tstype = TimeSeriesType(name="Price", description="Price Time Series")
    db.session.add(tstype)
    db.session.commit()

    # Attempt to create a TimeSeries from a DataFrame with no date
    try:
        ts = TimeSeries.from_dataframe(
            df=df,
            time_series_type=tstype.id,
            asset=asset.id
            # Not passing date_column because we expect the index to be a datetime or raise ValueError
        )
        raise AssertionError("Expected ValueError for missing date column but got no exception")
    except ValueError:
        pass
    except AssertionError as e:
        raise e
    except Exception as e:
        raise AssertionError(f"Expected ValueError for missing date column but got exception of type {type(e)}")



if __name__ == "__main__":
    test_timeseries_invalid_dataframe_raises()

================================================================================

FILE NAME: tests/fixtures.py

import pytest
import random
import string
from app.models import Asset, TimeSeriesType, TimeSeries, DataPoint
import datetime
import pandas as pd
from app import db
import numpy as np

# tests/fixtures.py

@pytest.fixture
def create_asset_and_type(app):
    """
    Fixture to create an Asset and a TimeSeriesType for testing.
    Returns their IDs, rather than returning detached instances.
    """
    with app.app_context():
        # Create a TimeSeriesType
        tstype = TimeSeriesType(name="Price", description="Price Time Series")
        db.session.add(tstype)
        db.session.commit()

        # Create an Asset
        asset = Asset(name="AST", description="Test Asset", is_tradable=True)
        db.session.add(asset)
        db.session.commit()

        return asset.id, tstype.id

@pytest.fixture
def populate_test_db(app):
    """
    Fixture to populate the test database with initial data.
    Accepts parameters for the number of assets and data points per time series.
    """

    def _populate_test_db(num_assets=1, num_data_points=10, start_date=datetime.date(2020, 1, 1)):
        """
        Internal function to populate the test database.
        
        Args:
            num_assets (int): Number of assets to create.
            num_data_points (int): Number of data points per time series.
            start_date (date or str): Start date for generating data points.
        """

        # Convert `start_date` to a `datetime.date` object if it's a string
        if isinstance(start_date, str):
            start_date = datetime.datetime.strptime(start_date, "%Y-%m-%d").date()

        def generate_random_name(length=3):
            """Generate a random name with the specified length."""
            return ''.join(random.choices(string.ascii_uppercase, k=length))

        def create_data_points(time_series_id, num_points, start_date):
            """Create random data points for a given time series."""
            data_points = []
            for i in range(num_points):
                date = start_date + datetime.timedelta(days=i)
                value = round(random.uniform(1000.0, 5000.0), 2)  # Random value between 1000 and 5000

                # Randomly decide whether to assign a `date_release` or leave it as None
                if random.choice([True, False]):
                    date_release = date + datetime.timedelta(days=1)
                else:
                    date_release = None

                data_point = DataPoint(
                    date=date,
                    value=value,
                    date_release=date_release,
                    time_series_id=time_series_id
                )
                data_points.append(data_point)
            return data_points

        with app.app_context():
            # Set a fixed random seed for reproducibility
            random.seed(42)

            # Create a TimeSeriesType
            time_series_type = TimeSeriesType(name='Price', description='Price Time Series')
            db.session.add(time_series_type)
            db.session.commit()

            # Generate the specified number of assets and time series
            for _ in range(num_assets):
                # Create a random asset
                asset_name = generate_random_name()
                asset = Asset(
                    name=asset_name,
                    description=f"{asset_name} Description",
                    is_tradable=bool(random.getrandbits(1))
                )
                db.session.add(asset)
                db.session.commit()

                # Create a random time series for the asset
                time_series_name = generate_random_name()
                time_series = TimeSeries(
                    name=f"{asset_name} {time_series_name}",
                    type_id=time_series_type.id,
                    asset_id=asset.id
                )
                db.session.add(time_series)
                db.session.commit()

                # Create random data points for the time series
                data_points = create_data_points(time_series.id, num_data_points, start_date)
                db.session.add_all(data_points)
                db.session.commit()

    return _populate_test_db

@pytest.fixture
def sample_df_single_column():
    """
    Returns a single-column DataFrame (with a DateTimeIndex).
    """
    rng = np.random.default_rng(seed=42)
    returns = rng.normal(0.0001, 0.01, 252)
    prices = 100 * (1 + np.cumsum(returns))
    dates = pd.date_range("2025-01-01", periods=252, freq="D")
    return pd.DataFrame({"price": prices}, index=dates)

@pytest.fixture
def sample_df_multiple_columns():
    """
    Returns a multi-column DataFrame (with a DateTimeIndex).
    """
    rng = np.random.default_rng(seed=42)
    returns = rng.normal(0.0001, 0.01, 252)
    prices = 100 * (1 + np.cumsum(returns))
    volumes = rng.integers(1000, 10000, 252)
    dates = pd.date_range("2025-01-01", periods=252, freq="D")
    return pd.DataFrame({"price": prices, "volume": volumes}, index=dates)

================================================================================



================================================================================

FILE NAME: scripts/app.txt

App files:

================================================================================

FILE NAME: app/models.py

from app import db
import pandas as pd
from sqlalchemy.sql import func
import datetime
import re

# Association table for many-to-many relationship between SeriesGroup and SeriesBase
seriesgroup_seriesbase = db.Table(
    'seriesgroup_seriesbase',
    db.Column('seriesgroup_id', db.Integer, db.ForeignKey('series_group.id'), primary_key=True),
    db.Column('seriesbase_id', db.Integer, db.ForeignKey('series_base.id'), primary_key=True)
)

class BaseModel(db.Model):
    __abstract__ = True

    def save(self, session=None, commit=True):
        """
        Saves the current instance to the database, ensuring any dependent objects are also saved.
        session (db.session): The SQLAlchemy session to use. If None, uses db.session.
        commit (bool): Whether or not to commit immediately.
        """
        if session is None:
            session = db.session

        self._save_dependencies(session)

        session.add(self)

        if commit:
            session.commit()

    def _save_dependencies(self, session):
        """
        Override this in child classes to save any dependencies (parents or children).
        Default implementation does nothing.
        """
        pass

class SeriesBase(BaseModel):
    __tablename__ = 'series_base'
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(100), nullable=False)
    description = db.Column(db.String(200))
    type = db.Column(db.String(50))  # Discriminator column

    date_create = db.Column(db.DateTime(timezone=True), server_default=func.now(), nullable=False)
    date_update = db.Column(db.DateTime(timezone=True), server_default=func.now(),
                            onupdate=func.now(), nullable=False)

    __mapper_args__ = {
        'polymorphic_identity': 'series_base',
        'polymorphic_on': type,
        'with_polymorphic': '*'
    }

    def __repr__(self):
        return f'<SeriesBase {self.name}>'

class SeriesGroup(SeriesBase):
    __tablename__ = 'series_group'
    id = db.Column(db.Integer, db.ForeignKey('series_base.id'), primary_key=True)
    series_code = db.Column(db.String(10), nullable=False, unique=True)

    # Self-referential relationship for nested SeriesGroups
    parent_id = db.Column(db.Integer, db.ForeignKey('series_group.id'), nullable=True)
    children = db.relationship(
        'SeriesGroup',
        backref=db.backref('parent', remote_side=[id]),
        lazy='dynamic'
    )

    # Many-to-many relationship with SeriesBase (TimeSeries and SeriesGroup)
    series = db.relationship(
        'SeriesBase',
        secondary=seriesgroup_seriesbase,
        backref=db.backref('series_groups', lazy='dynamic'),
        lazy='dynamic'
    )

    __mapper_args__ = {
        'polymorphic_identity': 'series_group',
    }

    def __repr__(self):
        return f'<SeriesGroup {self.name}>'

class TimeSeries(SeriesBase):
    __tablename__ = 'time_series'
    id = db.Column(db.Integer, db.ForeignKey('series_base.id'), primary_key=True)
    type_id = db.Column(db.Integer, db.ForeignKey('time_series_type.id'), nullable=False)
    delta_type = db.Column(db.String(10), nullable=True, default='pct')

    data_points = db.relationship('DataPoint', backref='time_series', lazy=True)

    __mapper_args__ = {
        'polymorphic_identity': 'time_series',
    }

    def _save_dependencies(self, session):
        """
        Ensures the related TimeSeriesType and DataPoint objects are also saved.
        """
        # Save the parent TimeSeriesType if it's new or modified
        if self.time_series_type:
            session.add(self.time_series_type)

        # Save child DataPoints
        for dp in self.data_points:
            session.add(dp)

    def to_dataframe(
            self,
            only_most_recent_per_date=True,
            filter_date_release_smaller_or_equal_to=None,
            include_date_release=False,
            include_date_create=False
        ):
        """
        Returns the TimeSeries data as a pandas DataFrame.
        """
        data = {
            'date': [dp.date for dp in self.data_points],
            'value': [dp.value for dp in self.data_points],
            'date_create': [dp.date_create for dp in self.data_points],
            'date_release': [dp.date_release for dp in self.data_points]
        }
        ts_dataframe = (
            pd.DataFrame(data)
            .sort_values(['date', 'date_release', 'date_create'])
        )
        if only_most_recent_per_date:
            ts_dataframe = ts_dataframe.drop_duplicates(subset=['date'])
        ts_dataframe = (
            ts_dataframe
            .set_index('date')
            .rename({'value': self.name}, axis=1)
        )
        if filter_date_release_smaller_or_equal_to is not None:
            if isinstance(filter_date_release_smaller_or_equal_to, str):
                try:
                    filter_date = pd.to_datetime(filter_date_release_smaller_or_equal_to)
                except ValueError:
                    raise ValueError("filter_date_release_smaller_or_equal_to must be a valid date string.")
            elif not isinstance(filter_date_release_smaller_or_equal_to, (datetime.datetime, datetime.date)):
                raise ValueError("filter_date_release_smaller_or_equal_to must be a valid date string.")
            else:
                filter_date = filter_date_release_smaller_or_equal_to
            ts_dataframe = ts_dataframe[ts_dataframe['date_release'] <= filter_date]
        
        if not include_date_release:
            ts_dataframe = ts_dataframe.drop(columns=['date_release'])
        if not include_date_create:
            ts_dataframe = ts_dataframe.drop(columns=['date_create'])
        return ts_dataframe

    @classmethod
    def from_dataframe(
        cls,
        df,
        series_groups=None,
        time_series_type=None,
        name=None,
        description=None,
        date_column=None
    ):
        """
        Creates one or more TimeSeries objects from a pandas DataFrame without saving to the database.

        Returns
        -------
        TimeSeries or List[TimeSeries]
            A single TimeSeries if exactly one column is processed, or a list of multiple TimeSeries objects.
        """
        if date_column is not None:
            if date_column not in df.columns:
                raise ValueError(f"Date column '{date_column}' not found in DataFrame.")
            df = df.set_index(date_column)

        if 'date' in [c.lower() for c in df.columns.tolist()]:
            df = df.set_index('date')
        try:
            df.index = pd.to_datetime(df.index)
        except ValueError:
            raise ValueError("The DataFrame must have a datetime index or specify a date_column.")
        df.index.name = 'date'

        if len(df.columns) == 1:
            if name is None:
                name = df.columns[0]

            # Allow series_groups to be optional
            if series_groups is not None:
                if isinstance(description, list):
                    description = description[0]
            else:
                if description is None:
                    description = ""
            
            return cls.build_time_series_object(
                df.iloc[:, 0].values,
                df.index,
                name,
                series_groups,  # Can be None
                time_series_type,
                description
            )
        else:
            if name is None:
                name = df.columns
            elif name and not isinstance(name, list):
                raise ValueError("Name must be a list if multiple columns are provided.")
            elif len(name) != len(df.columns):
                raise ValueError("Name list must match the number of columns in the DataFrame.")
            else:
                raise ValueError("Name must be provided as list")
            
            # Allow series_groups to be optional
            if series_groups is not None:
                if isinstance(series_groups, (str, int, SeriesGroup)):
                    series_groups = [series_groups] * len(df.columns)
                elif not isinstance(series_groups, list):
                    raise ValueError("SeriesGroups must be provided as list, string, or SeriesGroup instances")
                    
                if len(series_groups) != len(df.columns):
                    raise ValueError("SeriesGroups list must match the number of columns in the DataFrame.")
            else:
                series_groups = [None] * len(df.columns)  # No groups associated

            if isinstance(description, str) and description is not None:
                raise ValueError("Description must be a list if multiple columns are provided.")
            
            all_time_series = []
            for i, col in enumerate(df.columns):
                all_time_series.append(
                    cls.build_time_series_object(
                    df[col].values,
                    df.index,
                    name[i],
                    series_groups[i],
                    time_series_type,
                    description
                ))
            return all_time_series

    @classmethod
    def save_from_dataframe(cls,
        df,
        series_groups=None,
        time_series_type=None,
        name=None,
        description=None,
        date_column=None,
        value_columns=None,
    ):
        """
        Creates one or more TimeSeries objects from a pandas DataFrame and saves them to the database.

        Returns
        -------
        TimeSeries or List[TimeSeries]
            A single TimeSeries if exactly one column is processed, or a list of multiple TimeSeries objects.
        """
        time_series_objects = cls.from_dataframe(
            df,
            series_groups,
            time_series_type,
            name,
            description,
            date_column,
            value_columns
        )
        if isinstance(time_series_objects, list):
            for ts in time_series_objects:
                ts.save()
        else:
            time_series_objects.save()
        return True

    @classmethod
    def build_time_series_object(cls, values, dates, time_series_name, series_groups, time_series_type, description=None):
        """
        Build a TimeSeries object with DataPoint objects from provided values and dates.
        """
        data_points = []
        for i in range(len(values)):
            data_points.append(DataPoint(date=dates[i], value=values[i]))
        ts = cls(
            name=time_series_name,
            data_points=data_points
        )
        if description is not None:
            ts.description = description
        if isinstance(time_series_type, TimeSeriesType):
            ts.time_series_type = time_series_type
        else:
            ts.type_id = time_series_type

        # Associate with SeriesGroups if provided
        if series_groups:
            if isinstance(series_groups, SeriesGroup):
                ts.series_groups.append(series_groups)
            elif isinstance(series_groups, list):
                for sg in series_groups:
                    if sg is not None:
                        ts.series_groups.append(sg)
            else:
                raise ValueError("series_groups must be a SeriesGroup instance, list of SeriesGroup instances, or None")

        return ts

class DataPoint(BaseModel):
    __tablename__ = 'data_point'
    id = db.Column(db.Integer, primary_key=True)
    date = db.Column(db.Date, nullable=False)
    value = db.Column(db.Float, nullable=False)
    date_release = db.Column(db.Date, nullable=True)
    time_series_id = db.Column(db.Integer, db.ForeignKey('time_series.id'), nullable=False)

    date_create = db.Column(db.DateTime(timezone=True), server_default=func.now(), nullable=False)
    date_update = db.Column(
        db.DateTime(timezone=True), server_default=func.now(),
        onupdate=func.now(), nullable=False
    )

    def __repr__(self):
        return f'<DataPoint {self.date}: {self.value}>'

    def _save_dependencies(self, session):
        """
        Ensure the parent TimeSeries (and potentially its parent objects) are saved.
        """
        if self.time_series:
            # Make sure the parent TimeSeries saves its dependencies too.
            # This will also add the SeriesGroups and any other DataPoints.
            self.time_series._save_dependencies(session)
            session.add(self.time_series)


class TimeSeriesType(BaseModel):
    __tablename__ = 'time_series_type'
    id = db.Column(db.Integer, primary_key=True)
    name = db.Column(db.String(50), nullable=False, unique=True)
    description = db.Column(db.String(200))
    time_series = db.relationship('TimeSeries', backref='time_series_type', lazy=True)

    date_create = db.Column(
        db.DateTime(timezone=True), server_default=func.now(),
        nullable=False
    )
    date_update = db.Column(
        db.DateTime(timezone=True), server_default=func.now(),
        onupdate=func.now(), nullable=False
    )

    __mapper_args__ = {
        'polymorphic_identity': 'time_series_type',
    }

    def __repr__(self):
        return f'<TimeSeriesType {self.name}>'


================================================================================

FILE NAME: app/__init__.py

from flask import Flask
from flask_sqlalchemy import SQLAlchemy
from config import config  # Import the config dictionary
from flask_migrate import Migrate
import os

db = SQLAlchemy()
migrate = Migrate()

def create_app(config_name=None):
    app = Flask(__name__)
    
    # Determine the configuration to use
    if config_name is None:
        config_name = os.getenv('FLASK_ENV') or 'default'
    
    # Use the configuration class from the config dictionary
    app.config.from_object(config[config_name])
    
    # Initialize extensions
    db.init_app(app)
    migrate.init_app(app, db)
    
    # Import and register Blueprints
    from .routes import main
    app.register_blueprint(main)
    
    with app.app_context():
        from . import models
    
    return app

================================================================================

FILE NAME: app/routes.py

from flask import Blueprint

# Define the Blueprint
main = Blueprint('main', __name__)

@main.route('/')
def home():
    return "Welcome to the Investment Portfolio App!"

================================================================================



================================================================================

